"""
Agentä»»åŠ¡ç±»æ¨¡å—

å®ç°åŸºäºLangGraphçš„æ™ºèƒ½ä»£ç†å¯¹è¯ä»»åŠ¡ï¼Œæ”¯æŒæµå¼å“åº”ã€‚
"""

from typing import Dict, List, Optional, Any, AsyncIterator
from datetime import datetime

from .base_task import BaseConversationTask
from ..models import Message, GlobalContext
from ..langgraph import LangGraphManager
from ..services import LLMService
from ..config import get_logger


class AgentTask(BaseConversationTask):
    """æ™ºèƒ½ä»£ç†å¯¹è¯ä»»åŠ¡ï¼ˆæ”¯æŒæµå¼å“åº”ï¼‰"""
    
    def __init__(self, user_id: str, conversation_id: Optional[str] = None):
        """åˆå§‹åŒ–Agentä»»åŠ¡"""
        super().__init__(user_id, conversation_id, mode="agent")
        
        # åˆå§‹åŒ–æœåŠ¡
        self.llm_service = LLMService()
        
        # åˆå§‹åŒ–LangGraphç®¡ç†å™¨
        self.langgraph_manager = LangGraphManager()
        
        # å…¨å±€ä¸Šä¸‹æ–‡
        self.global_context = GlobalContext()
        
        # Agentæ‰§è¡ŒçŠ¶æ€
        self.current_agent = ""
        self.execution_steps: List[Dict[str, Any]] = []
        
        # Workflowå®ŒæˆçŠ¶æ€æ ‡è¯†
        self.workflow_completed = False
        self.final_answer_sent = False
    
    async def _generate_with_stream(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        system_message: Optional[str] = None,
        stage_name: str = ""
    ) -> str:
        """
        ä½¿ç”¨æµå¼å“åº”ç”ŸæˆLLMå›å¤ï¼ŒåŒæ—¶æ”¶é›†å®Œæ•´å“åº”ç”¨äºåç»­å¤„ç†
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            stage_name: é˜¶æ®µåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            å®Œæ•´çš„LLMå“åº”å†…å®¹
        """
        full_response = ""
        
        self.logger.info(f"å¼€å§‹{stage_name}æµå¼å“åº”", conversation_id=self.conversation_id)
        
        # ä½¿ç”¨æµå¼å“åº”
        async for chunk in self.llm_service.generate_stream_response(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            system_message=system_message,
            conversation_history=self.history.get_recent_messages(limit=5)
        ):
            # å®æ—¶å‘é€å†…å®¹ç‰‡æ®µç»™ç”¨æˆ·
            await self.emit_content(chunk)
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_response += chunk
        
        self.logger.info(f"å®Œæˆ{stage_name}æµå¼å“åº”", 
                        conversation_id=self.conversation_id,
                        response_length=len(full_response))
        
        return full_response
    
    async def execute(self) -> None:
        """æ‰§è¡ŒAgentå·¥ä½œæµçš„ä¸»è¦é€»è¾‘"""
        try:
            # è·å–ç”¨æˆ·æœ€æ–°é—®é¢˜
            user_messages = self.history.get_messages_by_role("user")
            if not user_messages:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é—®é¢˜")
            
            user_question = user_messages[-1].content
            
            # åˆå§‹åŒ–å…¨å±€ä¸Šä¸‹æ–‡
            self.global_context.user_question = user_question
            self.global_context.conversation_history = self.history.messages
            
            # æ‰§è¡ŒLangGraphå·¥ä½œæµï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
            await self._execute_agent_workflow_with_stream()
            
        except Exception as e:
            self.logger.error_with_context(
                e,
                {
                    "conversation_id": self.conversation_id,
                    "current_agent": self.current_agent,
                    "user_id": self.user_id
                }
            )
            await self.emit_error("AGENT_EXECUTION_ERROR", f"Agentæ‰§è¡Œé”™è¯¯: {str(e)}")
            raise
    
    async def _execute_agent_workflow_with_stream(self) -> None:
        """æ‰§è¡ŒAgentå·¥ä½œæµï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        try:
            # é˜¶æ®µ1ï¼šé—®é¢˜ç†è§£ä¸åˆ†æ
            await self._agent_stage_understand_question()
            
            # é˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’
            await self._agent_stage_plan_tasks()
            
            # é˜¶æ®µ3ï¼šæ‰§è¡Œä»»åŠ¡
            await self._agent_stage_execute_tasks()
            
            # é˜¶æ®µ4ï¼šç»“æœæ•´åˆ
            await self._agent_stage_integrate_results()
            
        except Exception as e:
            self.logger.error_with_context(e, {"stage": "agent_workflow"})
            raise
    
    async def _agent_stage_understand_question(self) -> None:
        """Agenté˜¶æ®µ1ï¼šé—®é¢˜ç†è§£ä¸åˆ†æ"""
        self.update_stage("understanding_question")
        self.current_agent = "QuestionAnalyzer"
        await self.emit_status("understanding_question", progress=0.1)
        
        user_question = self.global_context.user_question
        
        # æ„å»ºé—®é¢˜åˆ†ææç¤º
        analyze_prompt = f"""
        ä½œä¸ºä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œæˆ‘éœ€è¦æ·±å…¥ç†è§£ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š{user_question}
        
        è¯·åˆ†æè¿™ä¸ªé—®é¢˜çš„ï¼š
        1. æ ¸å¿ƒæ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ
        2. é—®é¢˜çš„å¤æ‚ç¨‹åº¦å¦‚ä½•ï¼Ÿ
        3. éœ€è¦å“ªäº›ç±»å‹çš„ä¿¡æ¯æ¥å›ç­”ï¼Ÿ
        4. æ˜¯å¦éœ€è¦å®æ—¶ä¿¡æ¯ï¼Ÿ
        5. é—®é¢˜çš„å…³é”®è¯å’Œæ¦‚å¿µ
        
        è¯·æä¾›è¯¦ç»†çš„åˆ†ææ€è·¯ï¼š
        """
        
        await self.emit_content("ğŸ¤– **QuestionAnalyzer**: æ­£åœ¨åˆ†æé—®é¢˜...")
        
        # ä½¿ç”¨æµå¼å“åº”è¿›è¡Œé—®é¢˜åˆ†æ
        analysis_result = await self._generate_with_stream(
            analyze_prompt,
            temperature=0.3,
            stage_name="é—®é¢˜åˆ†æ"
        )
        
        # ä¿å­˜åˆ†æç»“æœåˆ°å…¨å±€ä¸Šä¸‹æ–‡
        self.global_context.question_analysis = analysis_result
        
        await self.emit_status("understanding_question", status="completed", progress=0.25)
    
    async def _agent_stage_plan_tasks(self) -> None:
        """Agenté˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’"""
        self.update_stage("planning_tasks")
        self.current_agent = "TaskPlanner"
        await self.emit_status("planning_tasks", progress=0.3)
        
        # æ„å»ºä»»åŠ¡è§„åˆ’æç¤º
        plan_prompt = f"""
        åŸºäºé—®é¢˜åˆ†æï¼Œåˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚
        
        é—®é¢˜åˆ†æç»“æœï¼š
        {self.global_context.question_analysis}
        
        è¯·åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼š
        1. éœ€è¦æ‰§è¡Œå“ªäº›å…·ä½“ä»»åŠ¡ï¼Ÿ
        2. ä»»åŠ¡çš„ä¼˜å…ˆçº§å’Œä¾èµ–å…³ç³»ï¼Ÿ
        3. æ¯ä¸ªä»»åŠ¡çš„é¢„æœŸè¾“å‡ºï¼Ÿ
        4. æ•´ä½“çš„è§£å†³æ€è·¯ï¼Ÿ
        
        è¯·è¯¦ç»†è¯´æ˜æ‰§è¡Œç­–ç•¥ï¼š
        """
        
        await self.emit_content("\nğŸ—‚ï¸ **TaskPlanner**: æ­£åœ¨åˆ¶å®šæ‰§è¡Œè®¡åˆ’...")
        
        # ä½¿ç”¨æµå¼å“åº”è¿›è¡Œä»»åŠ¡è§„åˆ’
        planning_result = await self._generate_with_stream(
            plan_prompt,
            temperature=0.4,
            stage_name="ä»»åŠ¡è§„åˆ’"
        )
        
        # ä¿å­˜è§„åˆ’ç»“æœ
        self.global_context.task_plan = planning_result
        
        await self.emit_status("planning_tasks", status="completed", progress=0.5)
    
    async def _agent_stage_execute_tasks(self) -> None:
        """Agenté˜¶æ®µ3ï¼šæ‰§è¡Œä»»åŠ¡"""
        self.update_stage("executing_tasks")
        self.current_agent = "TaskExecutor"
        await self.emit_status("executing_tasks", progress=0.6)
        
        # æ„å»ºä»»åŠ¡æ‰§è¡Œæç¤º
        execute_prompt = f"""
        ç°åœ¨å¼€å§‹æ‰§è¡Œä»»åŠ¡ã€‚
        
        åŸå§‹é—®é¢˜ï¼š{self.global_context.user_question}
        é—®é¢˜åˆ†æï¼š{self.global_context.question_analysis}
        æ‰§è¡Œè®¡åˆ’ï¼š{self.global_context.task_plan}
        
        è¯·æŒ‰ç…§è®¡åˆ’æ‰§è¡Œä»»åŠ¡ï¼Œå¹¶æä¾›ï¼š
        1. æ¯ä¸ªä»»åŠ¡çš„æ‰§è¡Œè¿‡ç¨‹
        2. å‘ç°çš„å…³é”®ä¿¡æ¯
        3. é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
        4. ä¸­é—´ç»“æœå’Œæ€è€ƒè¿‡ç¨‹
        
        è¯·è¯¦ç»†å±•ç¤ºæ‰§è¡Œè¿‡ç¨‹ï¼š
        """
        
        await self.emit_content("\nâš™ï¸ **TaskExecutor**: æ­£åœ¨æ‰§è¡Œä»»åŠ¡...")
        
        # ä½¿ç”¨æµå¼å“åº”æ‰§è¡Œä»»åŠ¡
        execution_result = await self._generate_with_stream(
            execute_prompt,
            temperature=0.6,
            stage_name="ä»»åŠ¡æ‰§è¡Œ"
        )
        
        # ä¿å­˜æ‰§è¡Œç»“æœ
        self.global_context.execution_results = execution_result
        
        await self.emit_status("executing_tasks", status="completed", progress=0.8)
    
    async def _agent_stage_integrate_results(self) -> None:
        """Agenté˜¶æ®µ4ï¼šç»“æœæ•´åˆ"""
        self.update_stage("integrating_results")
        self.current_agent = "ResultIntegrator"
        await self.emit_status("integrating_results", progress=0.9)
        
        # æ„å»ºç»“æœæ•´åˆæç¤º
        integrate_prompt = f"""
        ç°åœ¨éœ€è¦æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›æœ€ç»ˆç­”æ¡ˆã€‚
        
        åŸå§‹é—®é¢˜ï¼š{self.global_context.user_question}
        é—®é¢˜åˆ†æï¼š{self.global_context.question_analysis}
        æ‰§è¡Œè®¡åˆ’ï¼š{self.global_context.task_plan}
        æ‰§è¡Œç»“æœï¼š{self.global_context.execution_results}
        
        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªï¼š
        1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆ
        2. é€»è¾‘æ¸…æ™°ã€ç»“æ„å®Œæ•´
        3. åŒ…å«å¿…è¦çš„è§£é‡Šå’Œä¾æ®
        4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
        
        æœ€ç»ˆç­”æ¡ˆï¼š
        """
        
        await self.emit_content("\nğŸ”„ **ResultIntegrator**: æ­£åœ¨æ•´åˆç»“æœ...")
        
        # ä½¿ç”¨æµå¼å“åº”ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = await self._generate_with_stream(
            integrate_prompt,
            temperature=0.7,
            stage_name="ç»“æœæ•´åˆ"
        )
        
        # ä¿å­˜æœ€ç»ˆç­”æ¡ˆ
        self.global_context.final_answer = final_answer
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        assistant_message = Message(
            role="assistant",
            content=final_answer,
            metadata={
                "mode": "agent",
                "stages": ["understanding", "planning", "executing", "integrating"]
            }
        )
        self.history.add_message(assistant_message)
        
        await self.emit_content("\nâœ… **ä»»åŠ¡å®Œæˆ**")
        await self.emit_status("integrating_results", status="completed", progress=1.0)
        
        self.workflow_completed = True
        self.final_answer_sent = True
    
    async def _process_langgraph_chunk(self, chunk: Dict[str, Any]) -> None:
        """å¤„ç†LangGraphæµå¼è¾“å‡ºå—"""
        try:
            # è§£æchunkå†…å®¹
            node_name = chunk.get("node", "")
            node_output = chunk.get("output", {})
            
            if node_name:
                self.current_agent = node_name
                self.update_stage(f"agent_{node_name}")
                
                # è®°å½•æ‰§è¡Œæ­¥éª¤
                step = {
                    "agent": node_name,
                    "timestamp": datetime.now().isoformat(),
                    "output": node_output
                }
                self.execution_steps.append(step)
                
                # å‘é€çŠ¶æ€æ›´æ–°
                await self.emit_status(
                    f"agent_{node_name}",
                    agent_name=node_name,
                    metadata={"step_count": len(self.execution_steps)}
                )
                
                # å¤„ç†ä¸åŒAgentçš„è¾“å‡º
                await self._handle_agent_output(node_name, node_output)
                
        except Exception as e:
            self.logger.error(f"å¤„ç†LangGraph chunkå¤±è´¥: {str(e)}")
    
    async def _handle_agent_output(self, agent_name: str, output: Dict[str, Any]) -> None:
        """å¤„ç†ç‰¹å®šAgentçš„è¾“å‡º"""
        try:
            if agent_name == "master_agent":
                await self._handle_master_agent_output(output)
            elif agent_name == "query_optimizer":
                await self._handle_query_optimizer_output(output)
            elif agent_name == "parallel_search":
                await self._handle_parallel_search_output(output)
            elif agent_name == "summary_agent":
                await self._handle_summary_agent_output(output)
            elif agent_name == "final_output":
                await self._handle_final_output_output(output)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†Agent {agent_name} è¾“å‡ºå¤±è´¥: {str(e)}")
    
    async def _handle_master_agent_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æ€»æ§åˆ¶è€…Agentè¾“å‡º"""
        decision = output.get("decision", "")
        reasoning = output.get("reasoning", "")
        
        if reasoning:
            await self.emit_content(f"ğŸ¤– æ€»æ§åˆ¶è€…åˆ†æ: {reasoning}")
        
        if decision == "continue":
            await self.emit_content("éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯ï¼Œå¯åŠ¨æ£€ç´¢æµç¨‹...")
        elif decision == "finish":
            await self.emit_content("ä¿¡æ¯å……è¶³ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    
    async def _handle_query_optimizer_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†é—®é¢˜ä¼˜åŒ–Agentè¾“å‡º"""
        optimized_queries = output.get("optimized_queries", {})
        
        if optimized_queries:
            await self.emit_content("ğŸ” é—®é¢˜ä¼˜åŒ–å®Œæˆï¼Œç”Ÿæˆä¸“é—¨åŒ–æŸ¥è¯¢:")
            for agent_type, query in optimized_queries.items():
                await self.emit_content(f"  â€¢ {agent_type}: {query}")
    
    async def _handle_parallel_search_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†å¹¶è¡Œæœç´¢è¾“å‡º"""
        search_results = output.get("search_results", {})
        
        if search_results:
            await self.emit_content("ğŸ“Š å¹¶è¡Œæ£€ç´¢å®Œæˆ:")
            for search_type, results in search_results.items():
                result_count = len(results) if isinstance(results, list) else 1
                await self.emit_content(f"  â€¢ {search_type}: è·å¾— {result_count} ä¸ªç»“æœ")
    
    async def _handle_summary_agent_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æ‘˜è¦Agentè¾“å‡º"""
        summaries = output.get("summaries", {})
        
        if summaries:
            await self.emit_content("ğŸ“ ä¿¡æ¯æ‘˜è¦ç”Ÿæˆå®Œæˆ:")
            for source, summary in summaries.items():
                if summary:
                    await self.emit_content(f"  â€¢ {source}: {summary[:100]}...")
    
    async def _handle_final_output_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æœ€ç»ˆè¾“å‡ºAgentè¾“å‡º"""
        final_answer = output.get("final_answer", "")
        
        if final_answer and not self.final_answer_sent:
            # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
            assistant_message = Message(
                role="assistant",
                content=final_answer,
                metadata={
                    "agent_mode": True,
                    "execution_steps": len(self.execution_steps),
                    "agents_used": list(set(step["agent"] for step in self.execution_steps))
                }
            )
            self.add_message(assistant_message)
            
            # å‘é€æœ€ç»ˆç­”æ¡ˆ
            await self.emit_content(final_answer)
            self.final_answer_sent = True
            
            # æ ‡è®°workflowå®Œæˆ
            self.workflow_completed = True
            
            # å‘é€å®ŒæˆçŠ¶æ€
            await self.emit_status(
                "final_output_completed",
                status="completed", 
                progress=1.0,
                metadata={
                    "final_answer_length": len(final_answer),
                    "total_steps": len(self.execution_steps)
                }
            )
    
    async def _process_final_result(self, final_state: Dict[str, Any]) -> None:
        """å¤„ç†æœ€ç»ˆç»“æœ"""
        try:
            final_answer = final_state.get("final_answer", "")
            
            # å¦‚æœè¿˜æ²¡æœ‰å‘é€æœ€ç»ˆç­”æ¡ˆï¼Œåˆ™å¤„ç†
            if final_answer and not self.final_answer_sent:
                # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
                assistant_message = Message(
                    role="assistant",
                    content=final_answer,
                    metadata={
                        "agent_mode": True,
                        "final_state": True,
                        "execution_steps": len(self.execution_steps)
                    }
                )
                self.add_message(assistant_message)
                
                # å‘é€æœ€ç»ˆç­”æ¡ˆ
                await self.emit_content(final_answer)
                self.final_answer_sent = True
            
            # æ ‡è®°workflowå®Œæˆ
            self.workflow_completed = True
            
            # å‘é€æœ€ç»ˆå®ŒæˆçŠ¶æ€
            await self.emit_status(
                "agent_workflow", 
                status="completed", 
                progress=1.0,
                metadata={
                    "workflow_completed": True,
                    "final_answer_length": len(final_answer),
                    "total_execution_steps": len(self.execution_steps)
                }
            )
            
            self.logger.info(
                "Agentå·¥ä½œæµæ‰§è¡Œå®Œæˆ",
                conversation_id=self.conversation_id,
                execution_steps=len(self.execution_steps),
                final_answer_length=len(final_answer),
                workflow_completed=self.workflow_completed
            )
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}")
            await self.emit_error(
                error_code="FINAL_RESULT_ERROR",
                error_message=f"å¤„ç†æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}"
            )
    
    def get_execution_summary(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        return {
            **self.get_conversation_summary(),
            "agent_mode": True,
            "current_agent": self.current_agent,
            "execution_steps": len(self.execution_steps),
            "agents_used": list(set(step["agent"] for step in self.execution_steps)),
            "global_context_summary": self.global_context.get_all_contexts_summary()
        }
