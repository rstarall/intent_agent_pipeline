"""
å·¥ä½œæµä»»åŠ¡ç±»æ¨¡å—

å®ç°å›ºå®šæµç¨‹çš„å¤šè½®å¯¹è¯ä»»åŠ¡ï¼ŒåŒ…å«5ä¸ªé˜¶æ®µçš„æ‰§è¡Œæµç¨‹ï¼š
é˜¶æ®µ0ï¼šé—®é¢˜æ‰©å†™ä¸ä¼˜åŒ–
é˜¶æ®µ1ï¼šé—®é¢˜åˆ†æä¸è§„åˆ’  
é˜¶æ®µ2ï¼šä»»åŠ¡åˆ†è§£ä¸è°ƒåº¦
é˜¶æ®µ3ï¼šå¹¶è¡Œä»»åŠ¡æ‰§è¡Œ
é˜¶æ®µ4ï¼šç»“æœæ•´åˆä¸å›ç­”
"""

import json
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Any

from .base_task import BaseConversationTask
from .prompts import (
    build_question_expansion_prompt,
    build_expert_analysis_prompt,
    build_universal_task_planning_prompt, 
    build_comprehensive_synthesis_prompt,
    build_knowledge_base_selection_prompt,
    PromptConfig
)
from ..models import Message, ParallelTasksConfig, TaskConfig, SearchResult
from ..services import (
    KnowledgeService, LightRagService, SearchService, LLMService
)


class WorkflowTask(BaseConversationTask):
    """å›ºå®šå·¥ä½œæµå¯¹è¯ä»»åŠ¡"""
    
    def __init__(self, user_id: str, conversation_id: Optional[str] = None):
        """åˆå§‹åŒ–å·¥ä½œæµä»»åŠ¡"""
        super().__init__(user_id, conversation_id, mode="workflow")
        
        # åˆå§‹åŒ–æœåŠ¡
        self.knowledge_service = KnowledgeService()
        self.lightrag_service = LightRagService()
        self.search_service = SearchService()
        self.llm_service = LLMService()
        
        # å·¥ä½œæµçŠ¶æ€
        self.expanded_question = ""  # æ‰©å†™åçš„é—®é¢˜
        self.optimized_question = ""
        self.parallel_tasks_config: Optional[ParallelTasksConfig] = None
        self.task_results: Dict[str, Any] = {}
        self.final_answer = ""
    
    async def execute(self) -> None:
        """æ‰§è¡Œå·¥ä½œæµçš„ä¸»è¦é€»è¾‘"""
        try:
            # DEBUG: æ‰“å°å½“å‰çš„çŸ¥è¯†åº“é…ç½®
            print("\n" + "="*80)
            print("[DEBUG] WorkflowTask.execute å¼€å§‹æ‰§è¡Œ:")
            print(f"  conversation_id: {self.conversation_id}")
            print(f"  knowledge_bases: {self.knowledge_bases}")
            print(f"  knowledge_api_url: {self.knowledge_api_url}")
            print("="*80 + "\n")
            
            # è·å–ç”¨æˆ·æœ€æ–°é—®é¢˜
            user_messages = self.history.get_messages_by_role("user")
            if not user_messages:
                await self.emit_error("NO_USER_MESSAGE", "æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é—®é¢˜")
                return
            
            user_question = user_messages[-1].content
            
            # é˜¶æ®µ0ï¼šé—®é¢˜æ‰©å†™ä¸ä¼˜åŒ–
            await self._stage_0_expand_question(user_question)
            
            # é˜¶æ®µ1ï¼šé—®é¢˜åˆ†æä¸è§„åˆ’
            await self._stage_1_analyze_question(self.expanded_question)
            
            # é˜¶æ®µ2ï¼šä»»åŠ¡åˆ†è§£ä¸è°ƒåº¦
            await self._stage_2_task_scheduling()
            
            # é˜¶æ®µ3ï¼šå¹¶è¡Œä»»åŠ¡æ‰§è¡Œ
            await self._stage_3_execute_tasks()
            
            # é˜¶æ®µ4ï¼šç»“æœæ•´åˆä¸å›ç­”
            await self._stage_4_generate_answer(user_question)
            
        except Exception as e:
            self.logger.error_with_context(
                e,
                {
                    "conversation_id": self.conversation_id,
                    "stage": self.current_stage,
                    "user_id": self.user_id
                }
            )
            await self.emit_error("WORKFLOW_ERROR", f"å·¥ä½œæµæ‰§è¡Œé”™è¯¯: {str(e)}")
            raise
    
    async def _generate_with_stream(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        system_message: Optional[str] = None,
        content_prefix: str = "",
        json_mode: bool = False
    ) -> str:
        """
        ä½¿ç”¨æµå¼å“åº”ç”ŸæˆLLMå›å¤ï¼ŒåŒæ—¶æ”¶é›†å®Œæ•´å“åº”ç”¨äºåç»­å¤„ç†
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            content_prefix: å†…å®¹å‰ç¼€ï¼ˆç”¨äºåŒºåˆ†ä¸åŒé˜¶æ®µï¼‰
            json_mode: æ˜¯å¦ä¸ºJSONæ¨¡å¼
            
        Returns:
            å®Œæ•´çš„LLMå“åº”å†…å®¹
        """
        full_response = ""
        
        # å¦‚æœæ˜¯JSONæ¨¡å¼ï¼Œæ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if json_mode and not system_message:
            system_message = "You are a helpful assistant that always responds with valid JSON. Never include any text before or after the JSON object."
        
        # ä½¿ç”¨æµå¼å“åº”
        async for chunk in self.llm_service.generate_stream_response(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            system_message=system_message,
            conversation_history=self.history.get_recent_messages(limit=5)
        ):
            # å®æ—¶å‘é€å†…å®¹ç‰‡æ®µç»™ç”¨æˆ·
            if content_prefix:
                await self.emit_content(f"{content_prefix}{chunk}")
            else:
                await self.emit_content(chunk)
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_response += chunk
        
        return full_response
    
    async def _stage_0_expand_question(self, user_question: str) -> None:
        """é˜¶æ®µ0ï¼šé—®é¢˜æ‰©å†™ä¸ä¼˜åŒ–"""
        self.update_stage("expanding_question")
        await self.emit_status("expanding_question", progress=0.05)
        await self.emit_content("ğŸ” **å¯åŠ¨é—®é¢˜æ‰©å†™ä¸ä¼˜åŒ–...**\n")
        
        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = self._build_history_context()
        
        # è·å–æœ€è¿‘çš„å†å²é—®é¢˜
        recent_questions = self._get_recent_user_questions()
        
        # ä½¿ç”¨é—®é¢˜æ‰©å†™æç¤ºè¯
        expansion_prompt = build_question_expansion_prompt(
            user_question, 
            history_context, 
            recent_questions
        )
        
        await self.emit_content("æ­£åœ¨åŸºäºå†å²ä¸Šä¸‹æ–‡è¿›è¡Œé—®é¢˜æ‰©å†™...\n")
        
        try:
            # ä½¿ç”¨generate_json_responseè·å–æ‰©å†™ç»“æœ
            expansion_data = await self.llm_service.generate_json_response(
                expansion_prompt,
                temperature=PromptConfig.EXPANSION_TEMPERATURE
            )
            
            # æå–æ‰©å†™åçš„é—®é¢˜
            self.expanded_question = expansion_data.get("expanded_question", user_question)
            expansion_reasoning = expansion_data.get("expansion_reasoning", "")
            context_relevance = expansion_data.get("context_relevance", "medium")
            original_intent = expansion_data.get("original_intent", "")
            
            # éªŒè¯æ‰©å†™è´¨é‡
            if not self.expanded_question or len(self.expanded_question.strip()) < PromptConfig.MIN_EXPANSION_LENGTH:
                self.expanded_question = user_question  # ä½¿ç”¨åŸé—®é¢˜ä½œä¸ºåå¤‡
                await self.emit_content("âš ï¸ é—®é¢˜æ‰©å†™å¼‚å¸¸ï¼Œä½¿ç”¨åŸé—®é¢˜ç»§ç»­æ‰§è¡Œ\n")
            
            # æ˜¾ç¤ºæ‰©å†™ç»“æœ
            await self.emit_content(f"**âœ¨ é—®é¢˜æ‰©å†™å®Œæˆ**\n")
            await self.emit_content(f"- **åŸå§‹é—®é¢˜**: {user_question}\n")
            await self.emit_content(f"- **æ‰©å†™åé—®é¢˜**: {self.expanded_question}\n")
            await self.emit_content(f"- **æ‰©å†™ç†ç”±**: {expansion_reasoning}\n")
            await self.emit_content(f"- **ä¸Šä¸‹æ–‡å…³è”åº¦**: {context_relevance}\n")
            await self.emit_content(f"- **ç”¨æˆ·æ„å›¾**: {original_intent}\n\n")
            
        except Exception as e:
            self.logger.error_with_context(e, {"stage": "expansion", "question": user_question})
            # å¦‚æœæ‰©å†™å¤±è´¥ï¼Œä½¿ç”¨åŸé—®é¢˜
            self.expanded_question = user_question
            await self.emit_content("âš ï¸ é—®é¢˜æ‰©å†™å¤±è´¥ï¼Œä½¿ç”¨åŸé—®é¢˜ç»§ç»­æ‰§è¡Œ\n")
        
        await self.emit_status("expanding_question", status="completed", progress=0.1)
    
    async def _stage_1_analyze_question(self, user_question: str) -> None:
        """é˜¶æ®µ1ï¼šä¸“å®¶çº§é—®é¢˜åˆ†æä¸è§„åˆ’"""
        self.update_stage("analyzing_question")
        await self.emit_status("analyzing_question", progress=0.15)
        await self.emit_content("ğŸ” **å¯åŠ¨ä¸“å®¶çº§é—®é¢˜åˆ†æ...**\n")
        
        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = self._build_history_context()
        
        # ä½¿ç”¨SOTAä¸“å®¶åˆ†ææç¤ºè¯
        analysis_prompt = build_expert_analysis_prompt(user_question, history_context)
        
        await self.emit_content("æ­£åœ¨è¿›è¡Œå¤šç»´åº¦ä¸“ä¸šåˆ†æï¼Œè¯·ç¨å€™...\n")
        
        try:
            # ä½¿ç”¨generate_json_responseè·å–ç»“æ„åŒ–åˆ†æç»“æœ
            analysis_data = await self.llm_service.generate_json_response(
                analysis_prompt,
                temperature=PromptConfig.ANALYSIS_TEMPERATURE
            )
            
            if analysis_data and "expert_analysis" in analysis_data:
                expert_analysis = analysis_data["expert_analysis"]
                
                # æ ¼å¼åŒ–æ˜¾ç¤ºä¸“å®¶åˆ†æç»“æœ
                await self.emit_content("## ğŸ¯ **ä¸“å®¶åˆ†æç»“æœ**\n")
                await self.emit_content(f"{expert_analysis}\n")
                
                # ä¿å­˜åˆ†æç»“æœä¾›åç»­é˜¶æ®µä½¿ç”¨
                self.optimized_question = user_question  # ä¿æŒåŸé—®é¢˜ï¼Œå› ä¸ºåˆ†æå·²ç»åŒ…å«äº†ä¼˜åŒ–æ€è·¯
                self.expert_analysis = expert_analysis
                
                await self.emit_content("\nâœ… **ä¸“å®¶åˆ†æå®Œæˆ** - å·²ç”Ÿæˆæ·±åº¦ä¸“ä¸šåˆ†æ\n")
                await self.emit_status("analyzing_question", status="completed", progress=0.25)
                
            else:
                # åˆ†ææ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜
                self.logger.warning("ä¸“å®¶åˆ†æè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸")
                self.optimized_question = user_question
                self.expert_analysis = f"åŸºäºé—®é¢˜ï¼š{user_question}ï¼Œéœ€è¦è¿›è¡Œå…¨é¢çš„ä¿¡æ¯æ£€ç´¢å’Œåˆ†æã€‚"
                await self.emit_content("\nâš ï¸ åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°æ ¼å¼é—®é¢˜ï¼Œå·²ä½¿ç”¨åŸå§‹é—®é¢˜ç»§ç»­å¤„ç†\n")
                await self.emit_status("analyzing_question", status="completed", progress=0.25)
                
        except Exception as e:
            # å¦‚æœä¸“å®¶åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜å’ŒåŸºç¡€åˆ†æ
            self.logger.warning(f"ä¸“å®¶åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}")
            self.optimized_question = user_question
            self.expert_analysis = f"é’ˆå¯¹ç”¨æˆ·é—®é¢˜ï¼š{user_question}ï¼Œéœ€è¦è¿›è¡Œå¤šè§’åº¦çš„ä¿¡æ¯æ”¶é›†å’Œä¸“ä¸šåˆ†æï¼Œä»¥æä¾›å…¨é¢å‡†ç¡®çš„å›ç­”ã€‚"
            await self.emit_content(f"\nâš ï¸ ä¸“å®¶åˆ†æè¿‡ç¨‹é‡åˆ°é—®é¢˜ï¼Œå·²åˆ‡æ¢åˆ°åŸºç¡€æ¨¡å¼ç»§ç»­å¤„ç†\n")
            await self.emit_status("analyzing_question", status="completed", progress=0.25)
    
    async def _stage_2_task_scheduling(self) -> None:
        """é˜¶æ®µ2ï¼šæ™ºèƒ½ä»»åŠ¡åˆ†è§£ä¸è°ƒåº¦"""
        self.update_stage("task_scheduling")
        await self.emit_status("task_scheduling", progress=0.3)
        await self.emit_content("ğŸ“‹ **å¯åŠ¨æ™ºèƒ½ä»»åŠ¡è§„åˆ’...**\n")
        
        # è·å–ä¸“å®¶åˆ†æç»“æœ
        expert_analysis = getattr(self, 'expert_analysis', 'éœ€è¦è¿›è¡Œå…¨é¢çš„ä¿¡æ¯æ£€ç´¢å’Œåˆ†æ')
        
        # æ„å»ºå†å²ä¸Šä¸‹æ–‡
        history_context = self._build_history_context()
        
        # ä½¿ç”¨é€šç”¨ä»»åŠ¡è§„åˆ’æç¤ºè¯
        planning_prompt = build_universal_task_planning_prompt(self.optimized_question, expert_analysis, history_context)
        
        await self.emit_content("æ­£åœ¨è®¾è®¡æœ€ä¼˜æ£€ç´¢ç­–ç•¥ï¼Œè¯·ç¨å€™...\n")
        
        try:
            # ä½¿ç”¨generate_json_responseè·å–ç»“æ„åŒ–ä»»åŠ¡é…ç½®
            schedule_data = await self.llm_service.generate_json_response(
                planning_prompt,
                temperature=PromptConfig.PLANNING_TEMPERATURE
            )
            
            if schedule_data and "tasks" in schedule_data and isinstance(schedule_data["tasks"], list):
                tasks_config = schedule_data["tasks"]
                
                # éªŒè¯ä»»åŠ¡é…ç½®æ ¼å¼
                valid_tasks = []
                for task in tasks_config:
                    if isinstance(task, dict) and "type" in task and "query" in task:
                        # ç¡®ä¿ä»»åŠ¡ç±»å‹æœ‰æ•ˆ
                        if task["type"] in ["online_search", "knowledge_search", "lightrag_search"]:
                            valid_tasks.append(TaskConfig(**task))
                        else:
                            self.logger.warning(f"æ— æ•ˆçš„ä»»åŠ¡ç±»å‹: {task.get('type')}")
                
                if valid_tasks:
                    # æ ¼å¼åŒ–æ˜¾ç¤ºä»»åŠ¡è§„åˆ’ç»“æœ
                    await self.emit_content("## ğŸ¯ **æ£€ç´¢ç­–ç•¥è§„åˆ’**\n")
                    
                    type_names = {
                        "online_search": "ğŸŒ åœ¨çº¿æœç´¢",
                        "knowledge_search": "ğŸ“š çŸ¥è¯†åº“æ£€ç´¢",
                        "lightrag_search": "ğŸ”— çŸ¥è¯†å›¾è°±"
                    }
                    
                    for i, task in enumerate(valid_tasks, 1):
                        type_name = type_names.get(task.type, task.type)
                        await self.emit_content(f"**{i}. {type_name}**\n")
                        await self.emit_content(f"   æŸ¥è¯¢ç­–ç•¥: {task.query}\n\n")
                    
                    self.parallel_tasks_config = ParallelTasksConfig(
                        tasks=valid_tasks,
                        max_concurrency=3,
                        timeout=60
                    )
                    
                    await self.emit_content(f"âœ… **ä»»åŠ¡è§„åˆ’å®Œæˆ** - å·²ç”Ÿæˆ {len(valid_tasks)} ä¸ªå¹¶è¡Œæ£€ç´¢ä»»åŠ¡\n")
                    
                    # å¦‚æœæœ‰çŸ¥è¯†åº“é…ç½®ï¼Œæ˜¾ç¤ºé€‰æ‹©çš„çŸ¥è¯†åº“
                    if self.knowledge_bases and any(task.type == "knowledge_search" for task in valid_tasks):
                        await self.emit_content("\nğŸ“š **çŸ¥è¯†åº“é…ç½®ï¼š**\n")
                        await self.emit_content("ç³»ç»Ÿå°†æ ¹æ®é—®é¢˜å†…å®¹æ™ºèƒ½é€‰æ‹©æœ€ç›¸å…³çš„çŸ¥è¯†åº“è¿›è¡Œæ£€ç´¢\n")
                        
                        # å¦‚æœä½¿ç”¨äº†è‡ªå®šä¹‰çš„çŸ¥è¯†åº“API URL
                        if self.knowledge_api_url:
                            await self.emit_content(f"ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰çŸ¥è¯†åº“API: {self.knowledge_api_url}\n")
                    
                    await self.emit_status("task_scheduling", status="completed", progress=0.4)
                else:
                    # æ²¡æœ‰æœ‰æ•ˆä»»åŠ¡ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                    self.logger.warning("æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ä»»åŠ¡é…ç½®")
                    self._use_default_task_config()
                    await self.emit_content("âš ï¸ ä»»åŠ¡é…ç½®éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ£€ç´¢ç­–ç•¥\n")
                    await self.emit_status("task_scheduling", status="completed", progress=0.4)
            else:
                # JSONæ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                self.logger.warning("ä»»åŠ¡è§„åˆ’è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸")
                self._use_default_task_config()
                await self.emit_content("âš ï¸ ä»»åŠ¡è§„åˆ’æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤æ£€ç´¢ç­–ç•¥\n")
                await self.emit_status("task_scheduling", status="completed", progress=0.4)
                
        except Exception as e:
            # å¦‚æœä»»åŠ¡è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            self.logger.warning(f"ä»»åŠ¡è§„åˆ’ç”Ÿæˆå¤±è´¥: {str(e)}")
            self._use_default_task_config()
            await self.emit_content(f"âš ï¸ ä»»åŠ¡è§„åˆ’è¿‡ç¨‹é‡åˆ°é—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤æ£€ç´¢ç­–ç•¥\n")
            await self.emit_status("task_scheduling", status="completed", progress=0.4)
    
    def _use_default_task_config(self) -> None:
        """ä½¿ç”¨é»˜è®¤ä»»åŠ¡é…ç½®"""
        default_tasks = [
            TaskConfig(type="online_search", query=self.optimized_question),
            TaskConfig(type="knowledge_search", query=self.optimized_question),
            TaskConfig(type="lightrag_search", query=self.optimized_question)
        ]
        
        self.parallel_tasks_config = ParallelTasksConfig(
            tasks=default_tasks,
            max_concurrency=3,
            timeout=60
        )
    
    async def _stage_3_execute_tasks(self) -> None:
        """é˜¶æ®µ3ï¼šå¹¶è¡Œä»»åŠ¡æ‰§è¡Œ"""
        self.update_stage("executing_tasks")
        await self.emit_status("executing_tasks", progress=0.5)
        await self.emit_content("æ­£åœ¨æ‰§è¡Œå¹¶è¡Œæ£€ç´¢ä»»åŠ¡...")
        
        if not self.parallel_tasks_config:
            raise ValueError("ä»»åŠ¡é…ç½®æœªç”Ÿæˆ")
        
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = []
        for task_config in self.parallel_tasks_config.tasks:
            if task_config.type == "online_search":
                task = self._execute_online_search(task_config.query)
            elif task_config.type == "knowledge_search":
                task = self._execute_knowledge_search(task_config.query)
            elif task_config.type == "lightrag_search":
                task = self._execute_lightrag_search(task_config.query)
            else:
                continue
            
            tasks.append((task_config.type, task))
        
        # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
        results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
        
        # å¤„ç†ç»“æœå¹¶å®æ—¶åé¦ˆ
        await self.emit_content("\n\nğŸ“Š **æ£€ç´¢ç»“æœï¼š**")
        
        # å®šä¹‰ä»»åŠ¡ç±»å‹çš„ä¸­æ–‡åç§°
        type_names = {
            "online_search": "åœ¨çº¿æœç´¢",
            "knowledge_search": "çŸ¥è¯†åº“æ£€ç´¢", 
            "lightrag_search": "çŸ¥è¯†å›¾è°±"
        }
        
        success_count = 0
        for i, (task_type, _) in enumerate(tasks):
            result = results[i]
            type_name = type_names.get(task_type, task_type)
            
            if isinstance(result, Exception):
                # å¤„ç†é”™è¯¯æƒ…å†µ
                error_msg = str(result)
                self.logger.error(f"ä»»åŠ¡ {task_type} æ‰§è¡Œå¤±è´¥: {error_msg}")
                self.task_results[task_type] = {"error": error_msg}
                
                # å‘å‰ç«¯å‘é€é”™è¯¯åé¦ˆ
                await self.emit_content(f"\nâŒ **{type_name}** - æ£€ç´¢å¤±è´¥")
                await self.emit_content(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
            else:
                # å¤„ç†æˆåŠŸæƒ…å†µ
                self.task_results[task_type] = result
                
                # è®¡ç®—ç»“æœæ•°é‡
                result_count = 0
                if isinstance(result, dict):
                    if "results" in result:
                        raw_results = result["results"]
                        if isinstance(raw_results, list):
                            # å¤„ç†åˆ—è¡¨æ ¼å¼çš„ç»“æœï¼ˆåœ¨çº¿æœç´¢ã€ä¼ ç»ŸçŸ¥è¯†åº“æœç´¢ï¼‰
                            result_count = len(raw_results)
                        elif isinstance(raw_results, dict) and "documents" in raw_results:
                            # å¤„ç†query_docçš„è¿”å›æ ¼å¼
                            docs = raw_results.get("documents", [])
                            if docs and isinstance(docs[0], list):
                                result_count = len(docs[0])
                        elif isinstance(raw_results, dict):
                            # å¦‚æœæ˜¯å…¶ä»–å­—å…¸æ ¼å¼ï¼Œå°è¯•ä»å¸¸è§å­—æ®µè·å–è®¡æ•°
                            if "data" in raw_results:
                                data = raw_results["data"]
                                result_count = len(data) if isinstance(data, list) else 1
                            elif raw_results:  # éç©ºå­—å…¸å°±è®¤ä¸ºæœ‰ç»“æœ
                                result_count = 1
                
                # å‘å‰ç«¯å‘é€æˆåŠŸåé¦ˆ
                if result_count > 0:
                    await self.emit_content(f"\nâœ… **{type_name}** - æ£€ç´¢æˆåŠŸ")
                    await self.emit_content(f"   è·å¾— {result_count} ä¸ªç»“æœ")
                    success_count += 1
                else:
                    # è™½ç„¶æŠ€æœ¯ä¸ŠæˆåŠŸäº†ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°ç»“æœ
                    await self.emit_content(f"\nâš ï¸ **{type_name}** - æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                    await self.emit_content(f"   æŸ¥è¯¢: {result.get('query', 'æœªçŸ¥')}")
                    self.logger.warning(f"{task_type} è¿”å›äº†ç©ºç»“æœ")
        
        # æ€»ç»“åé¦ˆ
        await self.emit_content(f"\n\nğŸ“ˆ **æ£€ç´¢æ€»ç»“ï¼š**")
        await self.emit_content(f"- æˆåŠŸ: {success_count}/{len(tasks)} ä¸ªä»»åŠ¡")
        await self.emit_content(f"- å¤±è´¥: {len(tasks) - success_count}/{len(tasks)} ä¸ªä»»åŠ¡")
        
        # ç”Ÿæˆç»“æ„åŒ–çš„æœç´¢ç»“æœæŠ¥å‘Š
        await self._generate_search_results_report()
        
        await self.emit_status("executing_tasks", status="completed", progress=0.8)
    
    async def _stage_4_generate_answer(self, user_question: str) -> None:
        """é˜¶æ®µ4ï¼šä¸“ä¸šç»¼åˆåˆ†æä¸è¯¦ç»†å›ç­”"""
        self.update_stage("generating_answer")
        await self.emit_status("generating_answer", progress=0.85)
        await self.emit_content("\n\n## ğŸ’¡ **ä¸“ä¸šç»¼åˆåˆ†æ**\n")
        
        try:
            # æ„å»ºæ£€ç´¢ç»“æœä¸Šä¸‹æ–‡å’Œå†å²ä¸Šä¸‹æ–‡
            results_context = self._build_results_context()
            history_context = self._build_history_context()
            
            # ä½¿ç”¨ç»¼åˆåˆ†ææç¤ºè¯æ¨¡æ¿
            synthesis_prompt = build_comprehensive_synthesis_prompt(
                user_question,
                self.expanded_question, 
                self.optimized_question, 
                results_context, 
                history_context
            )
            
            # ä½¿ç”¨æµå¼å“åº”ç”Ÿæˆæœ€ç»ˆå›ç­”
            self.final_answer = await self._generate_with_stream(
                synthesis_prompt,
                temperature=PromptConfig.SYNTHESIS_TEMPERATURE,
                max_tokens=PromptConfig.MAX_SYNTHESIS_TOKENS
            )
            
            # éªŒè¯å›ç­”è´¨é‡
            if not self.final_answer or len(self.final_answer.strip()) < 100:
                # å¦‚æœå›ç­”è¿‡çŸ­ï¼Œæä¾›åŸºç¡€å›ç­”
                basic_answer = self._generate_basic_answer(user_question, results_context)
                self.final_answer = basic_answer
                await self.emit_content(f"\nâš ï¸ ä¸“ä¸šåˆ†æç”Ÿæˆå¼‚å¸¸ï¼Œå·²æä¾›åŸºç¡€å›ç­”\n")
                await self.emit_content(basic_answer)
            
            # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²è®°å½•
            assistant_message = Message(
                role="assistant",
                content=self.final_answer,
                metadata={
                    "stage": "comprehensive_analysis", 
                    "sources": list(self.task_results.keys()),
                    "analysis_type": "expert_synthesis"
                }
            )
            self.history.add_message(assistant_message)
            
            await self.emit_status("generating_answer", status="completed", progress=1.0)
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆä¸“ä¸šåˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            self.logger.error(error_msg)
            
            # ç”Ÿæˆå¤‡ç”¨å›ç­”
            try:
                results_context = self._build_results_context()
                fallback_answer = self._generate_basic_answer(user_question, results_context)
                self.final_answer = fallback_answer
                
                await self.emit_content(f"\nâš ï¸ {error_msg}\n")
                await self.emit_content("å·²åˆ‡æ¢åˆ°åŸºç¡€åˆ†ææ¨¡å¼ï¼š\n\n")
                await self.emit_content(fallback_answer)
                
            except Exception as fallback_error:
                # æœ€åçš„å…œåº•æ–¹æ¡ˆ
                self.final_answer = "å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ— æ³•ä¸ºæ‚¨æä¾›å®Œæ•´çš„åˆ†æã€‚è¿™å¯èƒ½æ˜¯ç”±äºç³»ç»Ÿè´Ÿè½½æˆ–ç½‘ç»œé—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…é‡æ–°æè¿°æ‚¨çš„é—®é¢˜ã€‚"
                await self.emit_error("ANSWER_GENERATION_ERROR", self.final_answer)
    
    def _generate_basic_answer(self, user_question: str, results_context: str) -> str:
        """ç”ŸæˆåŸºç¡€å›ç­”ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        basic_answer = f"""
## åŸºç¡€åˆ†æå›ç­”

**æ‚¨çš„é—®é¢˜ï¼š** {user_question}

**åŸºäºæ£€ç´¢ä¿¡æ¯çš„å›ç­”ï¼š**

æ ¹æ®æˆ‘ä»¬æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œé’ˆå¯¹æ‚¨çš„é—®é¢˜ï¼Œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥å›ç­”ï¼š

### æ ¸å¿ƒä¿¡æ¯
{self._extract_key_information(results_context)}

### è¯¦ç»†è¯´æ˜
{self._extract_detailed_information(results_context)}

### å‚è€ƒæ¥æº
{self._extract_source_references(results_context)}

---
*æ³¨ï¼šè¿™æ˜¯åŸºç¡€åˆ†ææ¨¡å¼çš„å›ç­”ã€‚å¦‚éœ€æ›´æ·±å…¥çš„ä¸“ä¸šåˆ†æï¼Œè¯·é‡æ–°æé—®ã€‚*
"""
        return basic_answer
    
    def _extract_key_information(self, results_context: str) -> str:
        """ä»æ£€ç´¢ç»“æœä¸­æå–å…³é”®ä¿¡æ¯"""
        if not results_context or results_context.strip() == "æ— æ£€ç´¢ç»“æœ":
            return "æš‚æ—¶æ²¡æœ‰è·å–åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # ç®€å•æå–å‰300å­—ç¬¦ä½œä¸ºæ ¸å¿ƒä¿¡æ¯
        key_info = results_context[:300]
        if len(results_context) > 300:
            key_info += "..."
        
        return key_info
    
    def _extract_detailed_information(self, results_context: str) -> str:
        """ä»æ£€ç´¢ç»“æœä¸­æå–è¯¦ç»†ä¿¡æ¯"""
        if not results_context or results_context.strip() == "æ— æ£€ç´¢ç»“æœ":
            return "ç”±äºä¿¡æ¯è·å–é™åˆ¶ï¼Œæ— æ³•æä¾›è¯¦ç»†è¯´æ˜ã€‚å»ºè®®æ‚¨å°è¯•æ›´å…·ä½“çš„é—®é¢˜æè¿°æˆ–ç¨åå†è¯•ã€‚"
        
        # æå–æ›´å¤šå†…å®¹ä½œä¸ºè¯¦ç»†ä¿¡æ¯
        detailed_info = results_context[300:800] if len(results_context) > 300 else "è¯¦ç»†ä¿¡æ¯æ­£åœ¨å¤„ç†ä¸­..."
        
        return detailed_info
    
    def _extract_source_references(self, results_context: str) -> str:
        """ä»æ£€ç´¢ç»“æœä¸­æå–æ¥æºå¼•ç”¨"""
        sources = []
        
        # ç®€å•çš„æ¥æºæå–é€»è¾‘
        for task_type, result in self.task_results.items():
            if "error" not in result:
                type_name = {"online_search": "åœ¨çº¿æœç´¢", "knowledge_search": "çŸ¥è¯†åº“", "lightrag_search": "çŸ¥è¯†å›¾è°±"}.get(task_type, task_type)
                sources.append(f"- {type_name}: å·²æ£€ç´¢ç›¸å…³ä¿¡æ¯")
        
        return "\n".join(sources) if sources else "- ç³»ç»Ÿå†…éƒ¨çŸ¥è¯†åº“"
    
    async def _execute_online_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œåœ¨çº¿æœç´¢"""
        try:
            self.logger.info(f"å¼€å§‹æ‰§è¡Œåœ¨çº¿æœç´¢: {query}")
            results = await self.search_service.search_online(query)
            self.logger.info(f"åœ¨çº¿æœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return {"type": "online_search", "query": query, "results": results}
        except Exception as e:
            error_msg = f"åœ¨çº¿æœç´¢å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            return {"type": "online_search", "query": query, "error": error_msg}
    
    async def _execute_knowledge_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒçŸ¥è¯†åº“æœç´¢ï¼ˆåŒ…å«æ™ºèƒ½é€‰æ‹©çŸ¥è¯†åº“çš„å­é˜¶æ®µï¼‰"""
        try:
            self.logger.info(f"å¼€å§‹æ‰§è¡ŒçŸ¥è¯†åº“æœç´¢: {query}")
            
            # å¦‚æœæœ‰ç”¨æˆ·tokenï¼Œä½¿ç”¨æ–°çš„query_docæ–¹æ³•
            if hasattr(self, 'user_token') and self.user_token:
                # å­é˜¶æ®µï¼šæ™ºèƒ½é€‰æ‹©çŸ¥è¯†åº“
                collection_name = await self._select_knowledge_base(query)
                
                if not collection_name:
                    # å¦‚æœé€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    collection_name = "test"
                    self.logger.warning("çŸ¥è¯†åº“é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“: test")
                
                # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ä¸ä¼šä½¿ç”¨æ— æ•ˆçš„çŸ¥è¯†åº“åç§°
                valid_names = [kb.get('name') for kb in self.knowledge_bases] if self.knowledge_bases else []
                if collection_name not in valid_names and collection_name != "test":
                    self.logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆçš„çŸ¥è¯†åº“åç§° '{collection_name}'ï¼Œå¼ºåˆ¶ä½¿ç”¨ 'test'")
                    await self.emit_content(f"\nâš ï¸ æœ€ç»ˆéªŒè¯å‘ç°çŸ¥è¯†åº“åç§° '{collection_name}' æ— æ•ˆï¼Œå·²å¼ºåˆ¶ä½¿ç”¨é»˜è®¤åº“ 'test'")
                    collection_name = "test"
                
                self.logger.info(f"ä½¿ç”¨query_docæ–¹æ³•ï¼Œcollection: {collection_name}")
                
                # å°è¯•ä½¿ç”¨é€‰å®šçš„çŸ¥è¯†åº“ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°é»˜è®¤å€¼
                try:
                    results = await self.knowledge_service.query_doc_by_name(
                        token=self.user_token,
                        knowledge_base_name=collection_name,
                        query=query,
                        k=5,
                        api_url=self.knowledge_api_url
                    )
                    self.logger.info(f"çŸ¥è¯†åº“æœç´¢æˆåŠŸ (query_doc_by_name)")
                    return {"type": "knowledge_search", "query": query, "results": results, "collection_name": collection_name}
                except Exception as e:
                    # å¦‚æœæ˜¯collectionä¸å­˜åœ¨çš„é”™è¯¯æˆ–æœªæ‰¾åˆ°çŸ¥è¯†åº“ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“
                    error_str = str(e)
                    if ("Collection" in error_str and "does not exist" in error_str) or \
                       ("æœªæ‰¾åˆ°åç§°ä¸º" in error_str and "çš„çŸ¥è¯†åº“" in error_str):
                        self.logger.warning(f"çŸ¥è¯†åº“ {collection_name} ä¸å­˜åœ¨æˆ–æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“: test")
                        await self.emit_content(f"\nâš ï¸ çŸ¥è¯†åº“ {collection_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“")
                        
                        try:
                            results = await self.knowledge_service.query_doc_by_name(
                                token=self.user_token,
                                knowledge_base_name="test",
                                query=query,
                                k=5,
                                api_url=self.knowledge_api_url
                            )
                            self.logger.info(f"ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“æœç´¢æˆåŠŸ")
                            return {"type": "knowledge_search", "query": query, "results": results, "collection_name": "test"}
                        except Exception as fallback_error:
                            # å¦‚æœé»˜è®¤çŸ¥è¯†åº“ä¹Ÿå¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯
                            raise fallback_error
                    else:
                        # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                        raise
            else:
                # ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•ï¼Œä¼ é€’knowledge_api_url
                self.logger.info(f"ä½¿ç”¨search_cosmetics_knowledgeæ–¹æ³•")
                results = await self.knowledge_service.search_cosmetics_knowledge(
                    query=query,
                    api_url=self.knowledge_api_url
                )
                result_count = len(results) if isinstance(results, list) else 0
                self.logger.info(f"çŸ¥è¯†åº“æœç´¢æˆåŠŸï¼Œè·å¾— {result_count} ä¸ªç»“æœ")
                return {"type": "knowledge_search", "query": query, "results": results}
        except Exception as e:
            error_msg = f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            return {"type": "knowledge_search", "query": query, "error": error_msg}
    
    async def _select_knowledge_base(self, query: str) -> Optional[str]:
        """æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„çŸ¥è¯†åº“"""
        try:
            # æ‰“å°å½“å‰çš„çŸ¥è¯†åº“é…ç½®ï¼Œä¾¿äºè°ƒè¯•
            self.logger.info(f"å¼€å§‹çŸ¥è¯†åº“é€‰æ‹©æµç¨‹ï¼ŒæŸ¥è¯¢: {query}")
            self.logger.info(f"å½“å‰çŸ¥è¯†åº“é…ç½®: {self.knowledge_bases}")
            
            # å‘ç”¨æˆ·æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            await self.emit_content(f"\nğŸ” **çŸ¥è¯†åº“é€‰æ‹©è°ƒè¯•ä¿¡æ¯**")
            await self.emit_content(f"   æŸ¥è¯¢å†…å®¹: {query}")
            await self.emit_content(f"   å¯ç”¨çŸ¥è¯†åº“æ•°é‡: {len(self.knowledge_bases) if self.knowledge_bases else 0}")
            
            # å¦‚æœæ²¡æœ‰é…ç½®çŸ¥è¯†åº“ï¼Œç›´æ¥è¿”å›é»˜è®¤å€¼
            if not self.knowledge_bases or len(self.knowledge_bases) == 0:
                self.logger.info("æ²¡æœ‰é…ç½®çŸ¥è¯†åº“ï¼Œä½¿ç”¨é»˜è®¤å€¼: test")
                await self.emit_content(f"   æœªé…ç½®çŸ¥è¯†åº“ï¼Œä½¿ç”¨é»˜è®¤: test")
                return "test"
            
            # æ˜¾ç¤ºå¯ç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨
            kb_names = [kb.get('name', 'æœªçŸ¥') for kb in self.knowledge_bases]
            await self.emit_content(f"   å¯ç”¨çŸ¥è¯†åº“: {', '.join(kb_names)}")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªçŸ¥è¯†åº“ï¼Œç›´æ¥ä½¿ç”¨
            if len(self.knowledge_bases) == 1:
                selected_name = self.knowledge_bases[0].get('name', 'test')
                self.logger.info(f"åªæœ‰ä¸€ä¸ªçŸ¥è¯†åº“ï¼Œç›´æ¥é€‰æ‹©: {selected_name}")
                await self.emit_content(f"   ä»…æœ‰ä¸€ä¸ªçŸ¥è¯†åº“ï¼Œç›´æ¥é€‰æ‹©: {selected_name}")
                return selected_name
            
            # ä½¿ç”¨æ–°çš„çŸ¥è¯†åº“é€‰æ‹©æç¤ºè¯
            selection_prompt = build_knowledge_base_selection_prompt(query, self.knowledge_bases)
            
            # è°ƒç”¨LLMé€‰æ‹©çŸ¥è¯†åº“
            result = await self.llm_service.generate_json_response(
                selection_prompt,
                temperature=PromptConfig.SELECTION_TEMPERATURE
            )
            
            if result and isinstance(result, dict):
                selected_name = result.get("collection_name", "").strip()
                reason = result.get("reason", "")
                
                # éªŒè¯é€‰æ‹©çš„çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
                valid_names = [kb.get('name') for kb in self.knowledge_bases]
                self.logger.info(f"LLMè¿”å›çš„çŸ¥è¯†åº“åç§°: '{selected_name}', å¯ç”¨é€‰é¡¹: {valid_names}")
                
                # ä¸¥æ ¼éªŒè¯é€‰æ‹©çš„åç§°
                if selected_name and selected_name in valid_names:
                    self.logger.info(f"æ™ºèƒ½é€‰æ‹©çŸ¥è¯†åº“: {selected_name}, åŸå› : {reason}")
                    # å‘å‰ç«¯å‘é€é€‰æ‹©ç»“æœ
                    await self.emit_content(f"\nğŸ¯ **çŸ¥è¯†åº“é€‰æ‹©**: {selected_name}")
                    if reason:
                        await self.emit_content(f"   é€‰æ‹©åŸå› : {reason}")
                    return selected_name
                else:
                    # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†å¸¸è§çš„æ— æ•ˆåç§°
                    invalid_names = ["default", "default_kb", "é»˜è®¤", "default_collection"]
                    if selected_name in invalid_names:
                        self.logger.warning(f"LLMä½¿ç”¨äº†ç¦æ­¢çš„çŸ¥è¯†åº“åç§°: '{selected_name}'ï¼Œè¿™æ˜¯å¸¸è§çš„é”™è¯¯")
                        await self.emit_content(f"\nâš ï¸ ç³»ç»Ÿæ£€æµ‹åˆ°æ— æ•ˆçš„çŸ¥è¯†åº“åç§° '{selected_name}'")
                    else:
                        self.logger.warning(f"LLMé€‰æ‹©äº†æ— æ•ˆçš„çŸ¥è¯†åº“: '{selected_name}'ï¼Œå¯ç”¨é€‰é¡¹: {valid_names}")
                        await self.emit_content(f"\nâš ï¸ LLMé€‰æ‹©äº†æ— æ•ˆçš„çŸ¥è¯†åº“åç§° '{selected_name}'")
                    
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„çŸ¥è¯†åº“ä½œä¸ºå›é€€
                    fallback_kb = valid_names[0] if valid_names else "test"
                    self.logger.info(f"è‡ªåŠ¨å›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çŸ¥è¯†åº“: {fallback_kb}")
                    await self.emit_content(f"   å·²è‡ªåŠ¨é€‰æ‹©: {fallback_kb}")
                    return fallback_kb
            else:
                self.logger.warning("LLMæœªèƒ½è¿”å›æœ‰æ•ˆçš„çŸ¥è¯†åº“é€‰æ‹©ç»“æœ")
                # è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„çŸ¥è¯†åº“
                valid_names = [kb.get('name') for kb in self.knowledge_bases]
                fallback_kb = valid_names[0] if valid_names else "test"
                self.logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çŸ¥è¯†åº“ä½œä¸ºå›é€€: {fallback_kb}")
                await self.emit_content(f"\nâš ï¸ çŸ¥è¯†åº“é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€‰æ‹©: {fallback_kb}")
                return fallback_kb
                
        except Exception as e:
            self.logger.error(f"é€‰æ‹©çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    async def _execute_lightrag_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒLightRAGæœç´¢"""
        try:
            self.logger.info(f"å¼€å§‹æ‰§è¡ŒLightRAGæœç´¢: {query}")
            results = await self.lightrag_service.search_lightrag(query, mode="mix")
            self.logger.info(f"LightRAGæœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return {"type": "lightrag_search", "query": query, "results": results}
        except Exception as e:
            # æ›´å®‰å…¨çš„å¼‚å¸¸æ¶ˆæ¯æå–ï¼Œé¿å…è®¿é—®ä¸å­˜åœ¨çš„é”®
            try:
                error_msg = f"LightRAGæœç´¢å¤±è´¥: {str(e)}"
            except Exception as str_error:
                # å¦‚æœstr(e)å¤±è´¥ï¼Œæä¾›å¤‡ç”¨é”™è¯¯æ¶ˆæ¯
                error_msg = f"LightRAGæœç´¢å¤±è´¥: {type(e).__name__}å¼‚å¸¸ï¼Œè¯¦æƒ…: {repr(e)}"
            
            self.logger.error(error_msg)
            return {"type": "lightrag_search", "query": query, "error": error_msg}
    
    async def _generate_search_results_report(self) -> None:
        """ç”Ÿæˆç»“æ„åŒ–çš„æœç´¢ç»“æœæŠ¥å‘Š"""
        import json
        
        # æ„å»ºç»“æ„åŒ–çš„æœç´¢ç»“æœ
        search_report = {
            "timestamp": datetime.now().isoformat(),
            "query": self.optimized_question,
            "search_results": {
                "online_search": self._format_search_results("online_search"),
                "knowledge_search": self._format_search_results("knowledge_search"),
                "lightrag_search": self._format_search_results("lightrag_search")
            }
        }
        
        # å‘é€JSONç»“æ„
        await self.emit_content("\n\n## ğŸ“Š æ£€ç´¢ç»“æœ\n")
        await self.emit_content("```json\n" + json.dumps(search_report, ensure_ascii=False, indent=2) + "\n```")
    
    def _format_search_results(self, search_type: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–å•ä¸ªæœç´¢ç±»å‹çš„ç»“æœ"""
        result = self.task_results.get(search_type, {})
        
        if "error" in result:
            return {
                "status": "error",
                "error": result["error"],
                "query": result.get("query", ""),
                "results": []
            }
        
        # æå–æœç´¢ç»“æœ
        search_results = []
        raw_results = result.get("results", [])
        
        # å¤„ç†ä¸åŒç±»å‹çš„ç»“æœæ ¼å¼
        if isinstance(raw_results, list):
            for item in raw_results[:5]:  # é™åˆ¶æ˜¾ç¤ºå‰5ä¸ªç»“æœ
                if hasattr(item, 'to_dict'):
                    item_dict = item.to_dict()
                else:
                    item_dict = item if isinstance(item, dict) else {}
                
                formatted_result = {
                    "title": item_dict.get("title", "æ— æ ‡é¢˜"),
                    "content": item_dict.get("content", "")[:200] + "..." if len(item_dict.get("content", "")) > 200 else item_dict.get("content", ""),
                    "url": item_dict.get("url", ""),
                    "score": item_dict.get("score", 0.0)
                }
                search_results.append(formatted_result)
        elif isinstance(raw_results, dict) and "documents" in raw_results:
            # å¤„ç†query_docæ ¼å¼
            docs = raw_results.get("documents", [])
            if docs and isinstance(docs[0], list):
                for i, doc in enumerate(docs[0][:5]):
                    formatted_result = {
                        "title": f"æ–‡æ¡£ç‰‡æ®µ {i+1}",
                        "content": doc[:200] + "..." if len(doc) > 200 else doc,
                        "url": "",
                        "score": 1.0
                    }
                    search_results.append(formatted_result)
        
        return {
            "status": "success",
            "query": result.get("query", ""),
            "result_count": len(search_results),
            "collection_name": result.get("collection_name", ""),
            "results": search_results
        }
    
    def _generate_markdown_report_deprecated(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æœç´¢æŠ¥å‘Š"""
        md_lines = []
        
        # æ ‡é¢˜å’Œæ—¶é—´
        md_lines.append(f"**æŸ¥è¯¢é—®é¢˜**: {report['query']}")
        md_lines.append(f"**æŸ¥è¯¢æ—¶é—´**: {report['timestamp']}")
        md_lines.append("")
        
        # å„æœç´¢ç±»å‹çš„ç»“æœ
        type_names = {
            "online_search": "ğŸŒ åœ¨çº¿æœç´¢",
            "knowledge_search": "ğŸ“š çŸ¥è¯†åº“æ£€ç´¢",
            "lightrag_search": "ğŸ”— çŸ¥è¯†å›¾è°±"
        }
        
        for search_type, type_name in type_names.items():
            result_data = report["search_results"][search_type]
            md_lines.append(f"### {type_name}")
            
            if result_data["status"] == "error":
                md_lines.append(f"- **çŠ¶æ€**: âŒ å¤±è´¥")
                md_lines.append(f"- **é”™è¯¯**: {result_data['error']}")
            else:
                md_lines.append(f"- **çŠ¶æ€**: âœ… æˆåŠŸ")
                md_lines.append(f"- **æŸ¥è¯¢**: {result_data['query']}")
                md_lines.append(f"- **ç»“æœæ•°**: {result_data['result_count']}")
                if result_data.get("collection_name"):
                    md_lines.append(f"- **çŸ¥è¯†åº“**: {result_data['collection_name']}")
                
                if result_data["results"]:
                    md_lines.append("\n**TOP ç»“æœ**:")
                    for i, res in enumerate(result_data["results"], 1):
                        md_lines.append(f"\n{i}. **{res['title']}**")
                        if res['content']:
                            md_lines.append(f"   > {res['content']}")
                        if res.get('url'):
                            md_lines.append(f"   > é“¾æ¥: {res['url']}")
                        if res.get('score') > 0:
                            md_lines.append(f"   > ç›¸å…³åº¦: {res['score']:.2f}")
            
            md_lines.append("")
        
        return "\n".join(md_lines)
    
    def _build_history_context(self) -> str:
        """æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡"""
        recent_messages = self.history.get_recent_messages(limit=5)
        context_parts = []
        
        for msg in recent_messages:
            context_parts.append(f"{msg.role}: {msg.content}")
        
        return "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"
    
    def _get_recent_user_questions(self, limit: int = 5) -> list:
        """è·å–æœ€è¿‘çš„ç”¨æˆ·é—®é¢˜åˆ—è¡¨"""
        user_messages = self.history.get_messages_by_role("user")
        if not user_messages:
            return []
        
        # è·å–æœ€è¿‘çš„ç”¨æˆ·é—®é¢˜ï¼Œæ’é™¤å½“å‰é—®é¢˜
        recent_questions = []
        for msg in user_messages[-limit-1:-1]:  # æ’é™¤æœ€åä¸€æ¡ï¼ˆå½“å‰é—®é¢˜ï¼‰
            if msg.content.strip():
                recent_questions.append(msg.content.strip())
        
        return recent_questions
    
    async def _generate_json_with_fallback(
        self,
        prompt: str,
        schema: Optional[Dict[str, Any]] = None,
        temperature: float = 0.3,
        show_stream: bool = True
    ) -> Optional[Dict[str, Any]]:
        """
        ç”ŸæˆJSONå“åº”ï¼Œå¸¦æœ‰æµå¼æ˜¾ç¤ºå’Œå›é€€æœºåˆ¶
        
        Args:
            prompt: æç¤ºè¯
            schema: JSONæ¨¡å¼
            temperature: æ¸©åº¦å‚æ•°
            show_stream: æ˜¯å¦æ˜¾ç¤ºæµå¼è¾“å‡º
            
        Returns:
            è§£æåçš„JSONæ•°æ®ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            if show_stream:
                # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºç»™ç”¨æˆ·
                result = await self._generate_with_stream(prompt, temperature=temperature)
            else:
                # ç›´æ¥è°ƒç”¨generate_json_response
                return await self.llm_service.generate_json_response(
                    prompt=prompt,
                    schema=schema,
                    temperature=temperature
                )
            
            # å°è¯•è§£æç»“æœ
            try:
                return json.loads(result)
            except json.JSONDecodeError:
                # å°è¯•æå–JSONéƒ¨åˆ†
                import re
                json_match = re.search(r'\{.*\}', result, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                return None
                
        except Exception as e:
            self.logger.error(f"ç”ŸæˆJSONå“åº”å¤±è´¥: {e}")
            return None
    
    def _build_results_context(self) -> str:
        """æ„å»ºæ£€ç´¢ç»“æœä¸Šä¸‹æ–‡ï¼ˆä¼˜åŒ–æ ¼å¼ä»¥ä¾¿å¼•ç”¨ï¼‰"""
        context_parts = []
        
        # å®šä¹‰ä»»åŠ¡ç±»å‹çš„ä¸­æ–‡åç§°
        type_names = {
            "online_search": "åœ¨çº¿æœç´¢",
            "knowledge_search": "çŸ¥è¯†åº“æ£€ç´¢", 
            "lightrag_search": "çŸ¥è¯†å›¾è°±"
        }
        
        # å…¨å±€å¼•ç”¨è®¡æ•°å™¨
        ref_counter = 1
        
        for task_type, result in self.task_results.items():
            type_name = type_names.get(task_type, task_type)
            
            if "error" in result:
                context_parts.append(f"\nã€{type_name}ã€‘\nçŠ¶æ€ï¼šæ£€ç´¢å¤±è´¥\né”™è¯¯ä¿¡æ¯ï¼š{result['error']}\n")
            else:
                context_parts.append(f"\nã€{type_name}ã€‘")
                context_parts.append(f"æŸ¥è¯¢ï¼š{result.get('query', 'æœªçŸ¥')}")
                
                if "results" in result and isinstance(result["results"], list):
                    context_parts.append(f"ç»“æœæ•°é‡ï¼š{len(result['results'])}ä¸ª\n")
                    
                    # æ ¼å¼åŒ–æ¯ä¸ªç»“æœï¼Œä¾¿äºå¼•ç”¨
                    for item in result["results"]:
                        if hasattr(item, 'to_dict'):
                            item_dict = item.to_dict()
                        else:
                            item_dict = item if isinstance(item, dict) else {}
                        
                        # ä½¿ç”¨å…¨å±€å¼•ç”¨ç¼–å·
                        context_parts.append(f"[{ref_counter}] {type_name}ç»“æœ:")
                        context_parts.append(f"  æ ‡é¢˜ï¼š{item_dict.get('title', 'æ— æ ‡é¢˜')}")
                        
                        # é™åˆ¶å†…å®¹é•¿åº¦
                        content = item_dict.get('content', 'æ— å†…å®¹')
                        if len(content) > 300:
                            content = content[:300] + "..."
                        context_parts.append(f"  å†…å®¹ï¼š{content}")
                        
                        # ç‰¹åˆ«æ ‡æ³¨URLä¿¡æ¯ï¼ˆåœ¨çº¿æœç´¢å¿…é¡»æœ‰URLï¼‰
                        url = item_dict.get('url', '')
                        if url:
                            context_parts.append(f"  **URLï¼š{url}**")
                        elif task_type == "online_search":
                            context_parts.append(f"  URLï¼šæ— ï¼ˆæœç´¢ç»“æœæœªæä¾›é“¾æ¥ï¼‰")
                        
                        # æ·»åŠ æ¥æºä¿¡æ¯
                        if item_dict.get('source'):
                            context_parts.append(f"  æ¥æºç±»å‹ï¼š{item_dict['source']}")
                        
                        # æ·»åŠ å…ƒæ•°æ®ä¸­çš„é‡è¦ä¿¡æ¯
                        metadata = item_dict.get('metadata', {})
                        if metadata.get('engine'):
                            context_parts.append(f"  æœç´¢å¼•æ“ï¼š{metadata['engine']}")
                        if metadata.get('publishedDate'):
                            context_parts.append(f"  å‘å¸ƒæ—¶é—´ï¼š{metadata['publishedDate']}")
                        
                        context_parts.append("")  # ç©ºè¡Œåˆ†éš”
                        ref_counter += 1
        
        return "\n".join(context_parts) if context_parts else "æ— æ£€ç´¢ç»“æœ"
    
    def _make_serializable(self, obj: Any) -> Any:
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, SearchResult):
            return obj.to_dict()
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        else:
            return obj
