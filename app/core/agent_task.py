"""
Agentä»»åŠ¡ç±»æ¨¡å—

å®ç°åŸºäºLangGraphçš„æ™ºèƒ½ä»£ç†å¯¹è¯ä»»åŠ¡ï¼Œæ”¯æŒæµå¼å“åº”ã€‚
"""

from typing import Dict, List, Optional, Any, AsyncIterator
from datetime import datetime

from .base_task import BaseConversationTask
from ..models import Message, GlobalContext
from ..langgraph import LangGraphManager
from ..services import LLMService
from ..config import get_logger


class AgentTask(BaseConversationTask):
    """æ™ºèƒ½ä»£ç†å¯¹è¯ä»»åŠ¡ï¼ˆæ”¯æŒæµå¼å“åº”ï¼‰"""
    
    def __init__(self, user_id: str, conversation_id: Optional[str] = None):
        """åˆå§‹åŒ–Agentä»»åŠ¡"""
        super().__init__(user_id, conversation_id, mode="agent")
        
        # åˆå§‹åŒ–æœåŠ¡
        self.llm_service = LLMService()
        
        # åˆå§‹åŒ–LangGraphç®¡ç†å™¨
        self.langgraph_manager = LangGraphManager()
        
        # å…¨å±€ä¸Šä¸‹æ–‡
        self.global_context = GlobalContext()
        
        # Agentæ‰§è¡ŒçŠ¶æ€
        self.current_agent = ""
        self.execution_steps: List[Dict[str, Any]] = []
        
        # Workflowå®ŒæˆçŠ¶æ€æ ‡è¯†
        self.workflow_completed = False
        self.final_answer_sent = False
    
    async def _generate_with_stream(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        system_message: Optional[str] = None,
        stage_name: str = ""
    ) -> str:
        """
        ä½¿ç”¨æµå¼å“åº”ç”ŸæˆLLMå›å¤ï¼ŒåŒæ—¶æ”¶é›†å®Œæ•´å“åº”ç”¨äºåç»­å¤„ç†
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            stage_name: é˜¶æ®µåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            å®Œæ•´çš„LLMå“åº”å†…å®¹
        """
        full_response = ""
        
        self.logger.info(f"å¼€å§‹{stage_name}æµå¼å“åº”", conversation_id=self.conversation_id)
        
        # ä½¿ç”¨æµå¼å“åº”
        async for chunk in self.llm_service.generate_stream_response(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            system_message=system_message,
            conversation_history=self.history.get_recent_messages(limit=5)
        ):
            # å®æ—¶å‘é€å†…å®¹ç‰‡æ®µç»™ç”¨æˆ·
            await self.emit_content(chunk)
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_response += chunk
        
        self.logger.info(f"å®Œæˆ{stage_name}æµå¼å“åº”", 
                        conversation_id=self.conversation_id,
                        response_length=len(full_response))
        
        return full_response
    
    async def execute(self) -> None:
        """æ‰§è¡ŒAgentå·¥ä½œæµçš„ä¸»è¦é€»è¾‘"""
        try:
            # è·å–ç”¨æˆ·æœ€æ–°é—®é¢˜
            user_messages = self.history.get_messages_by_role("user")
            if not user_messages:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é—®é¢˜")
            
            user_question = user_messages[-1].content
            
            # åˆå§‹åŒ–å…¨å±€ä¸Šä¸‹æ–‡
            self.global_context.user_question = user_question
            self.global_context.conversation_history = self.history.messages
            
            # æ‰§è¡ŒLangGraphå·¥ä½œæµï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
            await self._execute_agent_workflow_with_stream()
            
        except Exception as e:
            self.logger.error_with_context(
                e,
                {
                    "conversation_id": self.conversation_id,
                    "current_agent": self.current_agent,
                    "user_id": self.user_id
                }
            )
            await self.emit_error("AGENT_EXECUTION_ERROR", f"Agentæ‰§è¡Œé”™è¯¯: {str(e)}")
            raise
    
    async def _execute_agent_workflow_with_stream(self) -> None:
        """æ‰§è¡ŒAgentå·¥ä½œæµï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        try:
            # é˜¶æ®µ1ï¼šé—®é¢˜ç†è§£ä¸åˆ†æ
            await self._agent_stage_understand_question()
            
            # é˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’
            await self._agent_stage_plan_tasks()
            
            # é˜¶æ®µ3ï¼šæ‰§è¡Œä»»åŠ¡
            await self._agent_stage_execute_tasks()
            
            # é˜¶æ®µ4ï¼šç»“æœæ•´åˆ
            await self._agent_stage_integrate_results()
            
        except Exception as e:
            self.logger.error_with_context(e, {"stage": "agent_workflow"})
            raise
    
    async def _agent_stage_understand_question(self) -> None:
        """Agenté˜¶æ®µ1ï¼šé—®é¢˜ç†è§£ä¸åˆ†æ"""
        self.update_stage("analyzing_question")
        self.current_agent = "QuestionAnalyzer"
        await self.emit_status("analyzing_question", progress=0.1)
        
        user_question = self.global_context.user_question
        
        # æ„å»ºçŸ¥è¯†åº“æè¿°
        kb_info = ""
        if self.knowledge_bases:
            kb_list = []
            for kb in self.knowledge_bases:
                kb_list.append(f"- {kb.get('name', 'æœªçŸ¥')}: {kb.get('description', 'æ— æè¿°')}")
            kb_info = "\n\nå¯ç”¨çŸ¥è¯†åº“ï¼š\n" + "\n".join(kb_list)
        else:
            kb_info = "\n\næ³¨æ„ï¼šæœªé…ç½®ç‰¹å®šçŸ¥è¯†åº“ï¼Œå°†ä½¿ç”¨é»˜è®¤æ£€ç´¢ç­–ç•¥"
        
        # æ„å»ºé—®é¢˜åˆ†ææç¤º
        analyze_prompt = f"""
        ä½œä¸ºä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œæˆ‘éœ€è¦æ·±å…¥ç†è§£ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š{user_question}
        {kb_info}
        
        è¯·åˆ†æè¿™ä¸ªé—®é¢˜çš„ï¼š
        1. æ ¸å¿ƒæ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿ
        2. é—®é¢˜çš„å¤æ‚ç¨‹åº¦å¦‚ä½•ï¼Ÿ
        3. éœ€è¦å“ªäº›ç±»å‹çš„ä¿¡æ¯æ¥å›ç­”ï¼Ÿ
        4. æ˜¯å¦éœ€è¦å®æ—¶ä¿¡æ¯ï¼Ÿ
        5. é—®é¢˜çš„å…³é”®è¯å’Œæ¦‚å¿µ
        6. åº”è¯¥ä½¿ç”¨å“ªäº›çŸ¥è¯†åº“ï¼Ÿ
        
        è¯·æä¾›è¯¦ç»†çš„åˆ†ææ€è·¯ï¼š
        """
        
        await self.emit_content("ğŸ¤– **QuestionAnalyzer**: æ­£åœ¨åˆ†æé—®é¢˜...")
        
        # ä½¿ç”¨æµå¼å“åº”è¿›è¡Œé—®é¢˜åˆ†æ
        analysis_result = await self._generate_with_stream(
            analyze_prompt,
            temperature=0.3,
            stage_name="é—®é¢˜åˆ†æ"
        )
        
        # ä¿å­˜åˆ†æç»“æœåˆ°å…¨å±€ä¸Šä¸‹æ–‡
        self.global_context.question_analysis = analysis_result
        
        await self.emit_status("analyzing_question", status="completed", progress=0.25)
    
    async def _agent_stage_plan_tasks(self) -> None:
        """Agenté˜¶æ®µ2ï¼šä»»åŠ¡è§„åˆ’"""
        self.update_stage("task_scheduling")
        self.current_agent = "TaskPlanner"
        await self.emit_status("task_scheduling", progress=0.3)
        
        # æ„å»ºä»»åŠ¡è§„åˆ’æç¤º
        plan_prompt = f"""
        åŸºäºé—®é¢˜åˆ†æï¼Œåˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚
        
        é—®é¢˜åˆ†æç»“æœï¼š
        {self.global_context.question_analysis}
        
        è¯·åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼š
        1. éœ€è¦æ‰§è¡Œå“ªäº›å…·ä½“ä»»åŠ¡ï¼Ÿ
        2. ä»»åŠ¡çš„ä¼˜å…ˆçº§å’Œä¾èµ–å…³ç³»ï¼Ÿ
        3. æ¯ä¸ªä»»åŠ¡çš„é¢„æœŸè¾“å‡ºï¼Ÿ
        4. æ•´ä½“çš„è§£å†³æ€è·¯ï¼Ÿ
        
        è¯·è¯¦ç»†è¯´æ˜æ‰§è¡Œç­–ç•¥ï¼š
        """
        
        await self.emit_content("\nğŸ—‚ï¸ **TaskPlanner**: æ­£åœ¨åˆ¶å®šæ‰§è¡Œè®¡åˆ’...")
        
        # å¦‚æœæœ‰çŸ¥è¯†åº“é…ç½®ï¼Œè¾“å‡ºçŸ¥è¯†åº“é€‰æ‹©ä¿¡æ¯
        if self.knowledge_bases:
            await self.emit_content("\nğŸ“š ç³»ç»Ÿå°†åŸºäºé—®é¢˜å†…å®¹æ™ºèƒ½é€‰æ‹©æœ€ç›¸å…³çš„çŸ¥è¯†åº“")
        
        # ä½¿ç”¨æµå¼å“åº”è¿›è¡Œä»»åŠ¡è§„åˆ’
        planning_result = await self._generate_with_stream(
            plan_prompt,
            temperature=0.4,
            stage_name="ä»»åŠ¡è§„åˆ’"
        )
        
        # ä¿å­˜è§„åˆ’ç»“æœ
        self.global_context.task_plan = planning_result
        
        await self.emit_status("task_scheduling", status="completed", progress=0.5)
    
    async def _agent_stage_execute_tasks(self) -> None:
        """Agenté˜¶æ®µ3ï¼šæ‰§è¡Œä»»åŠ¡"""
        self.update_stage("executing_tasks")
        self.current_agent = "TaskExecutor"
        await self.emit_status("executing_tasks", progress=0.6)
        
        # æ„å»ºä»»åŠ¡æ‰§è¡Œæç¤º
        execute_prompt = f"""
        ç°åœ¨å¼€å§‹æ‰§è¡Œä»»åŠ¡ã€‚
        
        åŸå§‹é—®é¢˜ï¼š{self.global_context.user_question}
        é—®é¢˜åˆ†æï¼š{self.global_context.question_analysis}
        æ‰§è¡Œè®¡åˆ’ï¼š{self.global_context.task_plan}
        
        è¯·æŒ‰ç…§è®¡åˆ’æ‰§è¡Œä»»åŠ¡ï¼Œå¹¶æä¾›ï¼š
        1. æ¯ä¸ªä»»åŠ¡çš„æ‰§è¡Œè¿‡ç¨‹
        2. å‘ç°çš„å…³é”®ä¿¡æ¯
        3. é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
        4. ä¸­é—´ç»“æœå’Œæ€è€ƒè¿‡ç¨‹
        
        è¯·è¯¦ç»†å±•ç¤ºæ‰§è¡Œè¿‡ç¨‹ï¼š
        """
        
        await self.emit_content("\nâš™ï¸ **TaskExecutor**: æ­£åœ¨æ‰§è¡Œä»»åŠ¡...")
        
        # ä½¿ç”¨æµå¼å“åº”æ‰§è¡Œä»»åŠ¡
        execution_result = await self._generate_with_stream(
            execute_prompt,
            temperature=0.6,
            stage_name="ä»»åŠ¡æ‰§è¡Œ"
        )
        
        # ä¿å­˜æ‰§è¡Œç»“æœ
        self.global_context.execution_results = execution_result
        
        await self.emit_status("executing_tasks", status="completed", progress=0.8)
    
    async def _agent_stage_integrate_results(self) -> None:
        """Agenté˜¶æ®µ4ï¼šç»“æœæ•´åˆ"""
        self.update_stage("response_generation")
        self.current_agent = "ResultIntegrator"
        await self.emit_status("response_generation", progress=0.9)
        
        # æ„å»ºæ£€ç´¢ç»“æœçš„è¯¦ç»†ä¸Šä¸‹æ–‡
        results_context = self._build_results_context()
        history_context = self._build_history_context()
        
        # æ„å»ºç»“æœæ•´åˆæç¤º
        integrate_prompt = f"""
        åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å…¨é¢å‡†ç¡®çš„å›ç­”ã€‚
        
        ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{self.global_context.user_question}
        
        é—®é¢˜åˆ†æç»“æœï¼š
        {self.global_context.question_analysis}
        
        æ£€ç´¢ç»“æœï¼š
        {results_context}
        
        å¯¹è¯å†å²ï¼š
        {history_context}
        
        åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å’Œåˆ†æç»“æœï¼Œè¯·æä¾›ä¸€ä¸ªå…¨é¢ã€æ·±å…¥ã€æœ‰æ´å¯ŸåŠ›çš„å›ç­”ã€‚
        
        **å›ç­”æ¡†æ¶**ï¼š
        
        1. **ç›´æ¥å›ç­”**ï¼ˆå¼€é—¨è§å±±ï¼‰ï¼š
           - å…ˆç”¨1-2å¥è¯ç›´æ¥å›ç­”ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜
           - ç„¶åå±•å¼€è¯¦ç»†è¯´æ˜ï¼Œå±‚å±‚æ·±å…¥
        
        2. **ä¿¡æ¯æ•´åˆä¸åˆ†æ**ï¼ˆä¸»ä½“éƒ¨åˆ†ï¼‰ï¼š
           - ç»¼åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼Œæ„å»ºå®Œæ•´çŸ¥è¯†ä½“ç³»
           - åˆ†æä¸åŒä¿¡æ¯ä¹‹é—´çš„å…³è”ã€äº’è¡¥æˆ–çŸ›ç›¾
           - æä¾›å¤šç»´åº¦çš„è§†è§’ï¼ˆå¦‚ç†è®ºä¸å®è·µã€ä¼˜åŠ¿ä¸å±€é™ç­‰ï¼‰
           - é€‚å½“åŠ å…¥èƒŒæ™¯çŸ¥è¯†å¸®åŠ©ç†è§£
        
        3. **æ·±å…¥æ¢è®¨**ï¼ˆæ ¹æ®é—®é¢˜ç±»å‹æ‰©å±•ï¼‰ï¼š
           - åŸç†æœºåˆ¶ï¼šè§£é‡Šäº‹ç‰©è¿ä½œçš„åº•å±‚é€»è¾‘
           - æ¯”è¾ƒåˆ†æï¼šå¯¹æ¯”ä¸åŒæ–¹æ¡ˆæˆ–è§‚ç‚¹çš„å¼‚åŒ
           - æ¡ˆä¾‹è¯´æ˜ï¼šç”¨å…·ä½“ä¾‹å­è¯´æ˜æŠ½è±¡æ¦‚å¿µ
           - è¶‹åŠ¿æ´å¯Ÿï¼šåˆ†æå½“å‰çŠ¶å†µå’Œæœªæ¥å¯èƒ½
           - å®è·µå»ºè®®ï¼šæä¾›å¯æ“ä½œçš„æŒ‡å¯¼æ„è§
        
        4. **å¼•ç”¨è§„èŒƒ**ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š
           - åœ¨é™ˆè¿°å…·ä½“äº‹å®æˆ–æ•°æ®æ—¶æ ‡æ³¨[1]ã€[2]ç­‰
           - å¼•ç”¨ç¼–å·å¿…é¡»ä¸æ£€ç´¢ç»“æœç¼–å·å¯¹åº”
           - åœ¨å›ç­”æœ«å°¾è®¾ç½®"**å‚è€ƒæ¥æºï¼š**"ä¸“åŒº
           - æ ¼å¼ï¼š[ç¼–å·] æ¥æºç±»å‹ - "æ ‡é¢˜" - å†…å®¹æ‘˜è¦ï¼ˆåœ¨çº¿æœç´¢éœ€åŒ…å«URLï¼‰
        
        5. **æ€»ç»“ä¸å»¶ä¼¸**ï¼ˆç”»é¾™ç‚¹ç›ï¼‰ï¼š
           - **æ ¸å¿ƒè¦ç‚¹**ï¼šç”¨bullet pointsæ€»ç»“2-3ä¸ªå…³é”®ä¿¡æ¯
           - **æ€è€ƒå»¶ä¼¸**ï¼šæå‡º1-2ä¸ªå€¼å¾—è¿›ä¸€æ­¥æ¢è®¨çš„é—®é¢˜
           - **çŸ¥è¯†è¾¹ç•Œ**ï¼šè¯šå®è¯´æ˜å“ªäº›æ–¹é¢ä¿¡æ¯æœ‰é™
           - **è¡ŒåŠ¨å»ºè®®**ï¼šå¦‚é€‚ç”¨ï¼Œç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®
        
        **å†™ä½œåŸåˆ™**ï¼š
        - ç»“æ„æ¸…æ™°ï¼šå–„ç”¨å°æ ‡é¢˜ã€ç¼–å·ã€æ®µè½åˆ’åˆ†
        - æ·±æµ…ç»“åˆï¼šä¸“ä¸šåˆ†æé…åˆé€šä¿—è§£é‡Š
        - è®ºæ®å……åˆ†ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½æœ‰ä¾æ®æ”¯æ’‘
        - æ€ç»´å¼€æ”¾ï¼šå±•ç°å¤šå…ƒè§†è§’ï¼Œé¿å…ç»å¯¹åŒ–è¡¨è¿°
        - ä»·å€¼å¯¼å‘ï¼šä¸ä»…å›ç­”"æ˜¯ä»€ä¹ˆ"ï¼Œæ›´æ¢è®¨"ä¸ºä»€ä¹ˆ"å’Œ"æ€ä¹ˆåŠ"
        
        **çº¢çº¿è¦æ±‚**ï¼š
        - æ‰€æœ‰ä¿¡æ¯å¿…é¡»æ¥è‡ªæ£€ç´¢ç»“æœï¼Œä¸å¾—å‡­ç©ºåˆ›é€ 
        - URLå¿…é¡»æ˜¯æ£€ç´¢ç»“æœä¸­çš„çœŸå®é“¾æ¥
        - é‡åˆ°ä¿¡æ¯å†²çªæ—¶æ˜ç¡®æŒ‡å‡ºå¹¶åˆ†æå¯èƒ½åŸå› 
        - ä¿æŒå­¦æœ¯è¯šä¿¡å’Œæ‰¹åˆ¤æ€§æ€ç»´
        
        æœ€ç»ˆç­”æ¡ˆï¼š
        """
        
        await self.emit_content("\nğŸ”„ **ResultIntegrator**: æ­£åœ¨æ•´åˆç»“æœ...")
        
        # ä½¿ç”¨æµå¼å“åº”ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = await self._generate_with_stream(
            integrate_prompt,
            temperature=0.7,
            stage_name="ç»“æœæ•´åˆ"
        )
        
        # ä¿å­˜æœ€ç»ˆç­”æ¡ˆ
        self.global_context.final_answer = final_answer
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        assistant_message = Message(
            role="assistant",
            content=final_answer,
            metadata={
                "mode": "agent",
                "stages": ["understanding", "planning", "executing", "integrating"],
                "search_types_used": list(self.task_results.keys()),
                "total_results": sum(len(r.get("results", [])) for r in self.task_results.values() if isinstance(r, dict))
            }
        )
        self.history.add_message(assistant_message)
        
        await self.emit_content("\nâœ… **ä»»åŠ¡å®Œæˆ**")
        await self.emit_status("response_generation", status="completed", progress=1.0)
        
        self.workflow_completed = True
        self.final_answer_sent = True
    
    async def _process_langgraph_chunk(self, chunk: Dict[str, Any]) -> None:
        """å¤„ç†LangGraphæµå¼è¾“å‡ºå—"""
        try:
            # è§£æchunkå†…å®¹
            node_name = chunk.get("node", "")
            node_output = chunk.get("output", {})
            
            if node_name:
                self.current_agent = node_name
                self.update_stage("agent_workflow")
                
                # è®°å½•æ‰§è¡Œæ­¥éª¤
                step = {
                    "agent": node_name,
                    "timestamp": datetime.now().isoformat(),
                    "output": node_output
                }
                self.execution_steps.append(step)
                
                # å‘é€çŠ¶æ€æ›´æ–°
                await self.emit_status(
                    "agent_workflow",
                    agent_name=node_name,
                    metadata={"step_count": len(self.execution_steps)}
                )
                
                # å¤„ç†ä¸åŒAgentçš„è¾“å‡º
                await self._handle_agent_output(node_name, node_output)
                
        except Exception as e:
            self.logger.error(f"å¤„ç†LangGraph chunkå¤±è´¥: {str(e)}")
    
    async def _handle_agent_output(self, agent_name: str, output: Dict[str, Any]) -> None:
        """å¤„ç†ç‰¹å®šAgentçš„è¾“å‡º"""
        try:
            if agent_name == "master_agent":
                await self._handle_master_agent_output(output)
            elif agent_name == "query_optimizer":
                await self._handle_query_optimizer_output(output)
            elif agent_name == "parallel_search":
                await self._handle_parallel_search_output(output)
            elif agent_name == "summary_agent":
                await self._handle_summary_agent_output(output)
            elif agent_name == "final_output":
                await self._handle_final_output_output(output)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†Agent {agent_name} è¾“å‡ºå¤±è´¥: {str(e)}")
    
    async def _handle_master_agent_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æ€»æ§åˆ¶è€…Agentè¾“å‡º"""
        decision = output.get("decision", "")
        reasoning = output.get("reasoning", "")
        
        if reasoning:
            await self.emit_content(f"ğŸ¤– æ€»æ§åˆ¶è€…åˆ†æ: {reasoning}")
        
        if decision == "continue":
            await self.emit_content("éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯ï¼Œå¯åŠ¨æ£€ç´¢æµç¨‹...")
        elif decision == "finish":
            await self.emit_content("ä¿¡æ¯å……è¶³ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    
    async def _handle_query_optimizer_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†é—®é¢˜ä¼˜åŒ–Agentè¾“å‡º"""
        optimized_queries = output.get("optimized_queries", {})
        
        if optimized_queries:
            await self.emit_content("ğŸ” é—®é¢˜ä¼˜åŒ–å®Œæˆï¼Œç”Ÿæˆä¸“é—¨åŒ–æŸ¥è¯¢:")
            for agent_type, query in optimized_queries.items():
                await self.emit_content(f"  â€¢ {agent_type}: {query}")
    
    async def _handle_parallel_search_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†å¹¶è¡Œæœç´¢è¾“å‡º"""
        search_results = output.get("search_results", {})
        
        if search_results:
            await self.emit_content("\nğŸ“Š **å¹¶è¡Œæ£€ç´¢ç»“æœï¼š**")
            
            # å®šä¹‰ä»»åŠ¡ç±»å‹çš„ä¸­æ–‡åç§°
            type_names = {
                "online_search": "åœ¨çº¿æœç´¢",
                "knowledge_search": "çŸ¥è¯†åº“æ£€ç´¢", 
                "lightrag_search": "çŸ¥è¯†å›¾è°±"
            }
            
            success_count = 0
            total_count = len(search_results)
            
            for search_type, results in search_results.items():
                type_name = type_names.get(search_type, search_type)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                if isinstance(results, dict) and "error" in results:
                    await self.emit_content(f"\nâŒ **{type_name}** - æ£€ç´¢å¤±è´¥")
                    await self.emit_content(f"   é”™è¯¯ä¿¡æ¯: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    self.logger.error(f"Agentæ¨¡å¼ - {search_type} æ£€ç´¢å¤±è´¥: {results.get('error')}")
                else:
                    # è®¡ç®—ç»“æœæ•°é‡
                    if isinstance(results, list):
                        result_count = len(results)
                    elif isinstance(results, dict) and "results" in results:
                        result_count = len(results.get("results", []))
                    else:
                        result_count = 0
                    
                    if result_count > 0:
                        await self.emit_content(f"\nâœ… **{type_name}** - æ£€ç´¢æˆåŠŸ")
                        await self.emit_content(f"   è·å¾— {result_count} ä¸ªç»“æœ")
                        success_count += 1
                        self.logger.info(f"Agentæ¨¡å¼ - {search_type} æ£€ç´¢æˆåŠŸï¼Œè·å¾— {result_count} ä¸ªç»“æœ")
                    else:
                        # è™½ç„¶æŠ€æœ¯ä¸ŠæˆåŠŸäº†ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°ç»“æœ
                        await self.emit_content(f"\nâš ï¸ **{type_name}** - æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                        self.logger.warning(f"Agentæ¨¡å¼ - {search_type} è¿”å›äº†ç©ºç»“æœ")
            
            # æ€»ç»“åé¦ˆ
            await self.emit_content(f"\nğŸ“ˆ **æ£€ç´¢æ€»ç»“ï¼š**")
            await self.emit_content(f"- æˆåŠŸ: {success_count}/{total_count} ä¸ªä»»åŠ¡")
            await self.emit_content(f"- å¤±è´¥: {total_count - success_count}/{total_count} ä¸ªä»»åŠ¡")
    
    async def _handle_summary_agent_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æ‘˜è¦Agentè¾“å‡º"""
        summaries = output.get("summaries", {})
        
        if summaries:
            await self.emit_content("ğŸ“ ä¿¡æ¯æ‘˜è¦ç”Ÿæˆå®Œæˆ:")
            for source, summary in summaries.items():
                if summary:
                    await self.emit_content(f"  â€¢ {source}: {summary[:100]}...")
    
    async def _handle_final_output_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æœ€ç»ˆè¾“å‡ºAgentè¾“å‡º"""
        final_answer = output.get("final_answer", "")
        
        if final_answer and not self.final_answer_sent:
            # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
            assistant_message = Message(
                role="assistant",
                content=final_answer,
                metadata={
                    "agent_mode": True,
                    "execution_steps": len(self.execution_steps),
                    "agents_used": list(set(step["agent"] for step in self.execution_steps))
                }
            )
            self.add_message(assistant_message)
            
            # å‘é€æœ€ç»ˆç­”æ¡ˆ
            await self.emit_content(final_answer)
            self.final_answer_sent = True
            
            # æ ‡è®°workflowå®Œæˆ
            self.workflow_completed = True
            
            # å‘é€å®ŒæˆçŠ¶æ€
            await self.emit_status(
                "completed",
                status="completed", 
                progress=1.0,
                metadata={
                    "final_answer_length": len(final_answer),
                    "total_steps": len(self.execution_steps)
                }
            )
    
    async def _process_final_result(self, final_state: Dict[str, Any]) -> None:
        """å¤„ç†æœ€ç»ˆç»“æœ"""
        try:
            final_answer = final_state.get("final_answer", "")
            
            # å¦‚æœè¿˜æ²¡æœ‰å‘é€æœ€ç»ˆç­”æ¡ˆï¼Œåˆ™å¤„ç†
            if final_answer and not self.final_answer_sent:
                # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
                assistant_message = Message(
                    role="assistant",
                    content=final_answer,
                    metadata={
                        "agent_mode": True,
                        "final_state": True,
                        "execution_steps": len(self.execution_steps)
                    }
                )
                self.add_message(assistant_message)
                
                # å‘é€æœ€ç»ˆç­”æ¡ˆ
                await self.emit_content(final_answer)
                self.final_answer_sent = True
            
            # æ ‡è®°workflowå®Œæˆ
            self.workflow_completed = True
            
            # å‘é€æœ€ç»ˆå®ŒæˆçŠ¶æ€
            await self.emit_status(
                "agent_workflow", 
                status="completed", 
                progress=1.0,
                metadata={
                    "workflow_completed": True,
                    "final_answer_length": len(final_answer),
                    "total_execution_steps": len(self.execution_steps)
                }
            )
            
            self.logger.info(
                "Agentå·¥ä½œæµæ‰§è¡Œå®Œæˆ",
                conversation_id=self.conversation_id,
                execution_steps=len(self.execution_steps),
                final_answer_length=len(final_answer),
                workflow_completed=self.workflow_completed
            )
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}")
            await self.emit_error(
                error_code="FINAL_RESULT_ERROR",
                error_message=f"å¤„ç†æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}"
            )
    
    def _use_default_task_config(self) -> None:
        """ä½¿ç”¨é»˜è®¤ä»»åŠ¡é…ç½®"""
        default_query = self.global_context.user_question or "é»˜è®¤æŸ¥è¯¢"
        self.planned_tasks = [
            TaskConfig(type="online_search", query=default_query),
            TaskConfig(type="knowledge_search", query=default_query),
            TaskConfig(type="lightrag_search", query=default_query)
        ]
    
    async def _execute_online_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œåœ¨çº¿æœç´¢"""
        try:
            self.logger.info(f"Agentæ¨¡å¼ - å¼€å§‹æ‰§è¡Œåœ¨çº¿æœç´¢: {query}")
            results = await self.search_service.search_online(query)
            self.logger.info(f"Agentæ¨¡å¼ - åœ¨çº¿æœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return {"type": "online_search", "query": query, "results": results}
        except Exception as e:
            error_msg = f"åœ¨çº¿æœç´¢å¤±è´¥: {str(e)}"
            self.logger.error(f"Agentæ¨¡å¼ - {error_msg}")
            return {"type": "online_search", "query": query, "error": error_msg}
    
    async def _execute_knowledge_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒçŸ¥è¯†åº“æœç´¢"""
        try:
            self.logger.info(f"Agentæ¨¡å¼ - å¼€å§‹æ‰§è¡ŒçŸ¥è¯†åº“æœç´¢: {query}")
            
            # å¦‚æœæœ‰ç”¨æˆ·tokenï¼Œä½¿ç”¨æ–°çš„query_doc_by_nameæ–¹æ³•
            if hasattr(self, 'user_token') and self.user_token:
                knowledge_base_name = "test"  # é»˜è®¤çŸ¥è¯†åº“åç§°
                self.logger.info(f"Agentæ¨¡å¼ - ä½¿ç”¨query_doc_by_nameæ–¹æ³•ï¼ŒçŸ¥è¯†åº“åç§°: {knowledge_base_name}")
                results = await self.knowledge_service.query_doc_by_name(
                    token=self.user_token,
                    knowledge_base_name=knowledge_base_name,
                    query=query,
                    k=5,
                    api_url=self.knowledge_api_url
                )
                self.logger.info(f"Agentæ¨¡å¼ - çŸ¥è¯†åº“æœç´¢æˆåŠŸ (query_doc_by_name)")
                return {"type": "knowledge_search", "query": query, "results": results}
            else:
                # ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
                self.logger.info(f"Agentæ¨¡å¼ - ä½¿ç”¨search_cosmetics_knowledgeæ–¹æ³•")
                results = await self.knowledge_service.search_cosmetics_knowledge(
                    query=query,
                    api_url=self.knowledge_api_url
                )
                result_count = len(results) if isinstance(results, list) else 0
                self.logger.info(f"Agentæ¨¡å¼ - çŸ¥è¯†åº“æœç´¢æˆåŠŸï¼Œè·å¾— {result_count} ä¸ªç»“æœ")
                return {"type": "knowledge_search", "query": query, "results": results}
        except Exception as e:
            error_msg = f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {str(e)}"
            self.logger.error(f"Agentæ¨¡å¼ - {error_msg}")
            return {"type": "knowledge_search", "query": query, "error": error_msg}
    
    async def _execute_lightrag_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒLightRAGæœç´¢"""
        try:
            self.logger.info(f"Agentæ¨¡å¼ - å¼€å§‹æ‰§è¡ŒLightRAGæœç´¢: {query}")
            results = await self.lightrag_service.search_lightrag(query, mode="mix")
            self.logger.info(f"Agentæ¨¡å¼ - LightRAGæœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return {"type": "lightrag_search", "query": query, "results": results}
        except Exception as e:
            # æ›´å®‰å…¨çš„å¼‚å¸¸æ¶ˆæ¯æå–ï¼Œé¿å…è®¿é—®ä¸å­˜åœ¨çš„é”®
            try:
                error_msg = f"LightRAGæœç´¢å¤±è´¥: {str(e)}"
            except Exception as str_error:
                # å¦‚æœstr(e)å¤±è´¥ï¼Œæä¾›å¤‡ç”¨é”™è¯¯æ¶ˆæ¯
                error_msg = f"LightRAGæœç´¢å¤±è´¥: {type(e).__name__}å¼‚å¸¸ï¼Œè¯¦æƒ…: {repr(e)}"
            
            self.logger.error(f"Agentæ¨¡å¼ - {error_msg}")
            return {"type": "lightrag_search", "query": query, "error": error_msg}
    
    def _build_execution_summary(self) -> str:
        """æ„å»ºæ‰§è¡Œç»“æœæ‘˜è¦"""
        summary_parts = []
        
        for task_type, result in self.task_results.items():
            if "error" in result:
                summary_parts.append(f"{task_type}: æ£€ç´¢å¤±è´¥ - {result['error']}")
            else:
                # ç»Ÿè®¡ç»“æœæ•°é‡
                if "results" in result and isinstance(result["results"], list):
                    count = len(result["results"])
                    summary_parts.append(f"{task_type}: æˆåŠŸè·å– {count} ä¸ªç»“æœ")
                    
                    # æå–å…³é”®ä¿¡æ¯
                    key_info = []
                    for item in result["results"][:3]:  # åªå–å‰3ä¸ª
                        if hasattr(item, 'title') and hasattr(item, 'content'):
                            content_preview = item.content[:100] + "..." if len(item.content) > 100 else item.content
                            key_info.append(f"  - {item.title}: {content_preview}")
                    
                    if key_info:
                        summary_parts.extend(key_info)
        
        return "\n\n".join(summary_parts)
    
    def _make_serializable(self, obj: Any) -> Any:
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, SearchResult):
            return obj.to_dict()
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        else:
            return obj
    
    def _build_results_context(self) -> str:
        """æ„å»ºæ£€ç´¢ç»“æœä¸Šä¸‹æ–‡ï¼ˆä¼˜åŒ–æ ¼å¼ä»¥ä¾¿å¼•ç”¨ï¼‰"""
        context_parts = []
        
        # å®šä¹‰ä»»åŠ¡ç±»å‹çš„ä¸­æ–‡åç§°
        type_names = {
            "online_search": "åœ¨çº¿æœç´¢",
            "knowledge_search": "çŸ¥è¯†åº“æ£€ç´¢", 
            "lightrag_search": "çŸ¥è¯†å›¾è°±"
        }
        
        # å…¨å±€å¼•ç”¨è®¡æ•°å™¨
        ref_counter = 1
        
        for task_type, result in self.task_results.items():
            type_name = type_names.get(task_type, task_type)
            
            if "error" in result:
                context_parts.append(f"\nã€{type_name}ã€‘\nçŠ¶æ€ï¼šæ£€ç´¢å¤±è´¥\né”™è¯¯ä¿¡æ¯ï¼š{result['error']}\n")
            else:
                context_parts.append(f"\nã€{type_name}ã€‘")
                context_parts.append(f"æŸ¥è¯¢ï¼š{result.get('query', 'æœªçŸ¥')}")
                
                if "results" in result and isinstance(result["results"], list):
                    context_parts.append(f"ç»“æœæ•°é‡ï¼š{len(result['results'])}ä¸ª\n")
                    
                    # æ ¼å¼åŒ–æ¯ä¸ªç»“æœï¼Œä¾¿äºå¼•ç”¨
                    for item in result["results"]:
                        if hasattr(item, 'to_dict'):
                            item_dict = item.to_dict()
                        else:
                            item_dict = item if isinstance(item, dict) else {}
                        
                        # ä½¿ç”¨å…¨å±€å¼•ç”¨ç¼–å·
                        context_parts.append(f"[{ref_counter}] {type_name}ç»“æœ:")
                        context_parts.append(f"  æ ‡é¢˜ï¼š{item_dict.get('title', 'æ— æ ‡é¢˜')}")
                        
                        # é™åˆ¶å†…å®¹é•¿åº¦
                        content = item_dict.get('content', 'æ— å†…å®¹')
                        if len(content) > 300:
                            content = content[:300] + "..."
                        context_parts.append(f"  å†…å®¹ï¼š{content}")
                        
                        # ç‰¹åˆ«æ ‡æ³¨URLä¿¡æ¯ï¼ˆåœ¨çº¿æœç´¢å¿…é¡»æœ‰URLï¼‰
                        url = item_dict.get('url', '')
                        if url:
                            context_parts.append(f"  **URLï¼š{url}**")
                        elif task_type == "online_search":
                            context_parts.append(f"  URLï¼šæ— ï¼ˆæœç´¢ç»“æœæœªæä¾›é“¾æ¥ï¼‰")
                        
                        # æ·»åŠ æ¥æºä¿¡æ¯
                        if item_dict.get('source'):
                            context_parts.append(f"  æ¥æºç±»å‹ï¼š{item_dict['source']}")
                        
                        # æ·»åŠ å…ƒæ•°æ®ä¸­çš„é‡è¦ä¿¡æ¯
                        metadata = item_dict.get('metadata', {})
                        if metadata.get('engine'):
                            context_parts.append(f"  æœç´¢å¼•æ“ï¼š{metadata['engine']}")
                        if metadata.get('publishedDate'):
                            context_parts.append(f"  å‘å¸ƒæ—¶é—´ï¼š{metadata['publishedDate']}")
                        
                        context_parts.append("")  # ç©ºè¡Œåˆ†éš”
                        ref_counter += 1
        
        return "\n".join(context_parts) if context_parts else "æ— æ£€ç´¢ç»“æœ"
    
    def _build_history_context(self) -> str:
        """æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡"""
        recent_messages = self.history.get_recent_messages(limit=5)
        context_parts = []
        
        for msg in recent_messages:
            context_parts.append(f"{msg.role}: {msg.content}")
        
        return "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"
    
    def get_execution_summary(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        return {
            **self.get_conversation_summary(),
            "agent_mode": True,
            "current_agent": self.current_agent,
            "execution_steps": len(self.execution_steps),
            "agents_used": list(set(step["agent"] for step in self.execution_steps)),
            "global_context_summary": self.global_context.get_all_contexts_summary(),
            "task_results_summary": {
                task_type: {
                    "has_error": "error" in result,
                    "result_count": len(result.get("results", [])) if "results" in result else 0
                }
                for task_type, result in self.task_results.items()
            }
        }
