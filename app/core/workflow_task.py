"""
å·¥ä½œæµä»»åŠ¡ç±»æ¨¡å—

å®ç°å›ºå®šæµç¨‹çš„å¤šè½®å¯¹è¯ä»»åŠ¡ï¼ŒåŒ…å«4ä¸ªé˜¶æ®µçš„æ‰§è¡Œæµç¨‹ã€‚
"""

import json
import asyncio
from typing import Dict, List, Optional, Any

from .base_task import BaseConversationTask
from ..models import Message, ParallelTasksConfig, TaskConfig, SearchResult
from ..services import (
    KnowledgeService, LightRagService, SearchService, LLMService
)


class WorkflowTask(BaseConversationTask):
    """å›ºå®šå·¥ä½œæµå¯¹è¯ä»»åŠ¡"""
    
    def __init__(self, user_id: str, conversation_id: Optional[str] = None):
        """åˆå§‹åŒ–å·¥ä½œæµä»»åŠ¡"""
        super().__init__(user_id, conversation_id, mode="workflow")
        
        # åˆå§‹åŒ–æœåŠ¡
        self.knowledge_service = KnowledgeService()
        self.lightrag_service = LightRagService()
        self.search_service = SearchService()
        self.llm_service = LLMService()
        
        # å·¥ä½œæµçŠ¶æ€
        self.optimized_question = ""
        self.parallel_tasks_config: Optional[ParallelTasksConfig] = None
        self.task_results: Dict[str, Any] = {}
        self.final_answer = ""
    
    async def execute(self) -> None:
        """æ‰§è¡Œå·¥ä½œæµçš„ä¸»è¦é€»è¾‘"""
        try:
            # è·å–ç”¨æˆ·æœ€æ–°é—®é¢˜
            user_messages = self.history.get_messages_by_role("user")
            if not user_messages:
                await self.emit_error("NO_USER_MESSAGE", "æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é—®é¢˜")
                return
            
            user_question = user_messages[-1].content
            
            # é˜¶æ®µ1ï¼šé—®é¢˜åˆ†æä¸è§„åˆ’
            await self._stage_1_analyze_question(user_question)
            
            # é˜¶æ®µ2ï¼šä»»åŠ¡åˆ†è§£ä¸è°ƒåº¦
            await self._stage_2_task_scheduling()
            
            # é˜¶æ®µ3ï¼šå¹¶è¡Œä»»åŠ¡æ‰§è¡Œ
            await self._stage_3_execute_tasks()
            
            # é˜¶æ®µ4ï¼šç»“æœæ•´åˆä¸å›ç­”
            await self._stage_4_generate_answer(user_question)
            
        except Exception as e:
            self.logger.error_with_context(
                e,
                {
                    "conversation_id": self.conversation_id,
                    "stage": self.current_stage,
                    "user_id": self.user_id
                }
            )
            await self.emit_error("WORKFLOW_ERROR", f"å·¥ä½œæµæ‰§è¡Œé”™è¯¯: {str(e)}")
            raise
    
    async def _generate_with_stream(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        system_message: Optional[str] = None,
        content_prefix: str = ""
    ) -> str:
        """
        ä½¿ç”¨æµå¼å“åº”ç”ŸæˆLLMå›å¤ï¼ŒåŒæ—¶æ”¶é›†å®Œæ•´å“åº”ç”¨äºåç»­å¤„ç†
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            content_prefix: å†…å®¹å‰ç¼€ï¼ˆç”¨äºåŒºåˆ†ä¸åŒé˜¶æ®µï¼‰
            
        Returns:
            å®Œæ•´çš„LLMå“åº”å†…å®¹
        """
        full_response = ""
        
        # ä½¿ç”¨æµå¼å“åº”
        async for chunk in self.llm_service.generate_stream_response(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            system_message=system_message,
            conversation_history=self.history.get_recent_messages(limit=5)
        ):
            # å®æ—¶å‘é€å†…å®¹ç‰‡æ®µç»™ç”¨æˆ·
            if content_prefix:
                await self.emit_content(f"{content_prefix}{chunk}")
            else:
                await self.emit_content(chunk)
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_response += chunk
        
        return full_response
    
    async def _stage_1_analyze_question(self, user_question: str) -> None:
        """é˜¶æ®µ1ï¼šé—®é¢˜åˆ†æä¸è§„åˆ’ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        self.update_stage("analyzing_question")
        await self.emit_status("analyzing_question", progress=0.1)
        await self.emit_content("æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")
        
        # æ„å»ºåˆ†ææç¤º
        history_context = self._build_history_context()
        
        analyze_prompt = f"""
        è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶è¿›è¡Œä¼˜åŒ–å’Œè§„åˆ’ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š{user_question}
        
        å¯¹è¯å†å²ï¼š{history_context}
        
        è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
        1. ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
        2. åŸºäºå¯¹è¯å†å²ä¼˜åŒ–é—®é¢˜è¡¨è¿°
        3. åˆ¶å®šå›ç­”è¯¥é—®é¢˜çš„æ‰§è¡Œè®¡åˆ’
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "optimized_question": "ä¼˜åŒ–åçš„é—®é¢˜",
            "analysis": "é—®é¢˜åˆ†æ",
            "plan": "æ‰§è¡Œè®¡åˆ’"
        }}
        """
        
        # ä½¿ç”¨æµå¼å“åº”ç”Ÿæˆåˆ†æç»“æœ
        await self.emit_content("\nğŸ” **åˆ†ææ€è·¯ï¼š**\n")
        analysis_result = await self._generate_with_stream(
            analyze_prompt,
            temperature=0.3
        )
        
        try:
            analysis_data = json.loads(analysis_result)
            self.optimized_question = analysis_data.get("optimized_question", user_question)
            
            await self.emit_content(f"\nâœ… **åˆ†æå®Œæˆ**")
            await self.emit_content(f"- ä¼˜åŒ–åé—®é¢˜: {self.optimized_question}")
            await self.emit_content(f"- åˆ†æç»“æœ: {analysis_data.get('analysis', '')}")
            await self.emit_status("analyzing_question", status="completed", progress=0.25)
            
        except json.JSONDecodeError:
            self.optimized_question = user_question
            await self.emit_content("\nâš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜è¿›è¡Œåç»­å¤„ç†")
    
    async def _stage_2_task_scheduling(self) -> None:
        """é˜¶æ®µ2ï¼šä»»åŠ¡åˆ†è§£ä¸è°ƒåº¦ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        self.update_stage("task_scheduling")
        await self.emit_status("task_scheduling", progress=0.3)
        await self.emit_content("æ­£åœ¨åˆ¶å®šæ£€ç´¢ç­–ç•¥...")
        
        # æ„å»ºä»»åŠ¡è°ƒåº¦æç¤º
        schedule_prompt = f"""
        åŸºäºä¼˜åŒ–åçš„é—®é¢˜ï¼Œç”Ÿæˆå¹¶è¡Œæ£€ç´¢ä»»åŠ¡é…ç½®ã€‚
        
        ä¼˜åŒ–åçš„é—®é¢˜ï¼š{self.optimized_question}
        
        å¯ç”¨çš„æ£€ç´¢ç±»å‹ï¼š
        1. online_search - åœ¨çº¿æœç´¢æœ€æ–°ä¿¡æ¯
        2. knowledge_search - åŒ–å¦†å“ä¸“ä¸šçŸ¥è¯†åº“æ£€ç´¢
        3. lightrag_search - LightRAGçŸ¥è¯†å›¾è°±æ£€ç´¢
        
        è¯·ä¸ºæ¯ç§æ£€ç´¢ç±»å‹ç”Ÿæˆåˆé€‚çš„æŸ¥è¯¢é—®é¢˜ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "tasks": [
                {{"type": "online_search", "query": "é’ˆå¯¹åœ¨çº¿æœç´¢ä¼˜åŒ–çš„é—®é¢˜"}},
                {{"type": "knowledge_search", "query": "é’ˆå¯¹çŸ¥è¯†åº“æ£€ç´¢ä¼˜åŒ–çš„é—®é¢˜"}},
                {{"type": "lightrag_search", "query": "é’ˆå¯¹LightRAGæ£€ç´¢ä¼˜åŒ–çš„é—®é¢˜"}}
            ]
        }}
        """
        
        # ä½¿ç”¨æµå¼å“åº”ç”Ÿæˆä»»åŠ¡é…ç½®
        await self.emit_content("\nğŸ“‹ **ä»»åŠ¡è§„åˆ’ï¼š**\n")
        schedule_result = await self._generate_with_stream(
            schedule_prompt,
            temperature=0.2
        )
        
        try:
            schedule_data = json.loads(schedule_result)
            tasks = [TaskConfig(**task) for task in schedule_data.get("tasks", [])]
            
            self.parallel_tasks_config = ParallelTasksConfig(
                tasks=tasks,
                max_concurrency=3,
                timeout=60
            )
            
            await self.emit_content(f"\nâœ… **ä»»åŠ¡è§„åˆ’å®Œæˆ** - å·²ç”Ÿæˆ {len(tasks)} ä¸ªå¹¶è¡Œæ£€ç´¢ä»»åŠ¡")
            await self.emit_status("task_scheduling", status="completed", progress=0.4)
            
        except (json.JSONDecodeError, Exception) as e:
            # ä½¿ç”¨é»˜è®¤ä»»åŠ¡é…ç½®
            default_tasks = [
                TaskConfig(type="online_search", query=self.optimized_question),
                TaskConfig(type="knowledge_search", query=self.optimized_question),
                TaskConfig(type="lightrag_search", query=self.optimized_question)
            ]
            
            self.parallel_tasks_config = ParallelTasksConfig(
                tasks=default_tasks,
                max_concurrency=3,
                timeout=60
            )
            
            await self.emit_content(f"\nâš ï¸ ä»»åŠ¡é…ç½®è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            await self.emit_status("task_scheduling", status="completed", progress=0.4)
    
    async def _stage_3_execute_tasks(self) -> None:
        """é˜¶æ®µ3ï¼šå¹¶è¡Œä»»åŠ¡æ‰§è¡Œ"""
        self.update_stage("executing_tasks")
        await self.emit_status("executing_tasks", progress=0.5)
        await self.emit_content("æ­£åœ¨æ‰§è¡Œå¹¶è¡Œæ£€ç´¢ä»»åŠ¡...")
        
        if not self.parallel_tasks_config:
            raise ValueError("ä»»åŠ¡é…ç½®æœªç”Ÿæˆ")
        
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = []
        for task_config in self.parallel_tasks_config.tasks:
            if task_config.type == "online_search":
                task = self._execute_online_search(task_config.query)
            elif task_config.type == "knowledge_search":
                task = self._execute_knowledge_search(task_config.query)
            elif task_config.type == "lightrag_search":
                task = self._execute_lightrag_search(task_config.query)
            else:
                continue
            
            tasks.append((task_config.type, task))
        
        # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
        results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, (task_type, _) in enumerate(tasks):
            result = results[i]
            if isinstance(result, Exception):
                self.logger.error(f"ä»»åŠ¡ {task_type} æ‰§è¡Œå¤±è´¥: {str(result)}")
                self.task_results[task_type] = {"error": str(result)}
            else:
                self.task_results[task_type] = result
        
        await self.emit_content(f"å¹¶è¡Œæ£€ç´¢å®Œæˆï¼Œè·å¾— {len(self.task_results)} ä¸ªç»“æœ")
        await self.emit_status("executing_tasks", status="completed", progress=0.8)
    
    async def _stage_4_generate_answer(self, user_question: str) -> None:
        """é˜¶æ®µ4ï¼šç»“æœæ•´åˆä¸å›ç­”ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
        self.update_stage("generating_answer")
        await self.emit_status("generating_answer", progress=0.9)
        await self.emit_content("æ­£åœ¨æ•´åˆä¿¡æ¯å¹¶ç”Ÿæˆå›ç­”...")
        
        try:
            # æ„å»ºæ•´åˆæç¤º
            results_context = self._build_results_context()
            history_context = self._build_history_context()
            
            integration_prompt = f"""
            åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å…¨é¢å‡†ç¡®çš„å›ç­”ã€‚
            
            ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{user_question}
            ä¼˜åŒ–åé—®é¢˜ï¼š{self.optimized_question}
            
            æ£€ç´¢ç»“æœï¼š
            {results_context}
            
            å¯¹è¯å†å²ï¼š
            {history_context}
            
            è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå…¨é¢ã€å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚è¦æ±‚ï¼š
            1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
            2. æ•´åˆå¤šæºä¿¡æ¯
            3. ä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
            4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜
            """
            
            # ä½¿ç”¨æµå¼å“åº”ç”Ÿæˆæœ€ç»ˆå›ç­”
            await self.emit_content("\nğŸ’¡ **æ­£åœ¨ç”Ÿæˆå›ç­”ï¼š**\n")
            self.final_answer = await self._generate_with_stream(
                integration_prompt,
                temperature=0.7
            )
            
            # å¦‚æœæ²¡æœ‰è·å¾—æœ‰æ•ˆå›ç­”ï¼Œæä¾›é»˜è®¤å›ç­”
            if not self.final_answer or len(self.final_answer.strip()) < 10:
                self.final_answer = "å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ— æ³•ä¸ºæ‚¨æä¾›å®Œæ•´çš„å›ç­”ã€‚è¿™å¯èƒ½æ˜¯ç”±äºç½‘ç»œé—®é¢˜æˆ–æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åå†è¯•ã€‚"
                await self.emit_content(f"\nâš ï¸ {self.final_answer}")
            
            # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
            assistant_message = Message(
                role="assistant",
                content=self.final_answer,
                metadata={"stage": "final_answer", "sources": list(self.task_results.keys())}
            )
            self.history.add_message(assistant_message)
            
            await self.emit_content(f"\nâœ… **å›ç­”ç”Ÿæˆå®Œæˆ**")
            await self.emit_status("generating_answer", status="completed", progress=1.0)
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            await self.emit_error("ANSWER_GENERATION_ERROR", error_msg)
            self.final_answer = f"æŠ±æ­‰ï¼Œ{error_msg}"
    
    async def _execute_online_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œåœ¨çº¿æœç´¢"""
        try:
            results = await self.search_service.search_online(query)
            return {"type": "online_search", "query": query, "results": results}
        except Exception as e:
            return {"type": "online_search", "query": query, "error": str(e)}
    
    async def _execute_knowledge_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒçŸ¥è¯†åº“æœç´¢"""
        try:
            # å¦‚æœæœ‰ç”¨æˆ·tokenï¼Œä½¿ç”¨æ–°çš„query_docæ–¹æ³•
            if hasattr(self, 'user_token') and self.user_token:
                # ä½¿ç”¨é»˜è®¤çš„collection_nameï¼Œå¯ä»¥åœ¨é…ç½®ä¸­è®¾ç½®
                collection_name = "cosmetics_knowledge"  # å¯é…ç½®
                results = await self.knowledge_service.query_doc(
                    token=self.user_token,
                    collection_name=collection_name,
                    query=query,
                    k=5
                )
                return {"type": "knowledge_search", "query": query, "results": results}
            else:
                # ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•
                results = await self.knowledge_service.search_cosmetics_knowledge(query)
                return {"type": "knowledge_search", "query": query, "results": results}
        except Exception as e:
            return {"type": "knowledge_search", "query": query, "error": str(e)}
    
    async def _execute_lightrag_search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡ŒLightRAGæœç´¢"""
        try:
            results = await self.lightrag_service.search_lightrag(query, mode="mix")
            return {"type": "lightrag_search", "query": query, "results": results}
        except Exception as e:
            return {"type": "lightrag_search", "query": query, "error": str(e)}
    
    def _build_history_context(self) -> str:
        """æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡"""
        recent_messages = self.history.get_recent_messages(limit=5)
        context_parts = []
        
        for msg in recent_messages:
            context_parts.append(f"{msg.role}: {msg.content}")
        
        return "\n".join(context_parts) if context_parts else "æ— å†å²å¯¹è¯"
    
    def _build_results_context(self) -> str:
        """æ„å»ºæ£€ç´¢ç»“æœä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for task_type, result in self.task_results.items():
            if "error" in result:
                context_parts.append(f"{task_type}: æ£€ç´¢å¤±è´¥ - {result['error']}")
            else:
                # å¤„ç†åŒ…å«SearchResultå¯¹è±¡çš„ç»“æœ
                serializable_result = self._make_serializable(result)
                context_parts.append(f"{task_type}: {json.dumps(serializable_result, ensure_ascii=False)}")
        
        return "\n\n".join(context_parts) if context_parts else "æ— æ£€ç´¢ç»“æœ"
    
    def _make_serializable(self, obj: Any) -> Any:
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, SearchResult):
            return obj.to_dict()
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        else:
            return obj
