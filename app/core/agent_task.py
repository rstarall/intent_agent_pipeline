"""
Agentä»»åŠ¡ç±»æ¨¡å—

å®ç°åŸºäºLangGraphçš„æ™ºèƒ½ä»£ç†å¯¹è¯ä»»åŠ¡ã€‚
"""

from typing import Dict, List, Optional, Any, AsyncIterator
from datetime import datetime

from .base_task import BaseConversationTask
from ..models import Message, GlobalContext
from ..langgraph import LangGraphManager
from ..config import get_logger


class AgentTask(BaseConversationTask):
    """æ™ºèƒ½ä»£ç†å¯¹è¯ä»»åŠ¡"""
    
    def __init__(self, user_id: str, conversation_id: Optional[str] = None):
        """åˆå§‹åŒ–Agentä»»åŠ¡"""
        super().__init__(user_id, conversation_id, mode="agent")
        
        # åˆå§‹åŒ–LangGraphç®¡ç†å™¨
        self.langgraph_manager = LangGraphManager()
        
        # å…¨å±€ä¸Šä¸‹æ–‡
        self.global_context = GlobalContext()
        
        # Agentæ‰§è¡ŒçŠ¶æ€
        self.current_agent = ""
        self.execution_steps: List[Dict[str, Any]] = []
    
    async def execute(self) -> None:
        """æ‰§è¡ŒAgentå·¥ä½œæµçš„ä¸»è¦é€»è¾‘"""
        try:
            # è·å–ç”¨æˆ·æœ€æ–°é—®é¢˜
            user_messages = self.history.get_messages_by_role("user")
            if not user_messages:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é—®é¢˜")
            
            user_question = user_messages[-1].content
            
            # åˆå§‹åŒ–å…¨å±€ä¸Šä¸‹æ–‡
            self.global_context.user_question = user_question
            self.global_context.conversation_history = self.history.messages
            
            # æ‰§è¡ŒLangGraphå·¥ä½œæµ
            await self._execute_agent_workflow()
            
        except Exception as e:
            self.logger.error_with_context(
                e,
                {
                    "conversation_id": self.conversation_id,
                    "current_agent": self.current_agent,
                    "user_id": self.user_id
                }
            )
            raise
    
    async def _execute_agent_workflow(self) -> None:
        """æ‰§è¡ŒAgentå·¥ä½œæµ"""
        self.update_stage("agent_workflow")
        await self.emit_status("agent_workflow", progress=0.1)
        await self.emit_content("å¯åŠ¨æ™ºèƒ½ä»£ç†å·¥ä½œæµ...")
        
        # å‡†å¤‡LangGraphçŠ¶æ€
        initial_state = {
            "user_question": self.global_context.user_question,
            "conversation_history": [msg.to_dict() for msg in self.global_context.conversation_history],
            "online_search_results": [],
            "knowledge_search_results": [],
            "lightrag_results": [],
            "current_stage": "master_agent",
            "final_answer": "",
            "metadata": {
                "conversation_id": self.conversation_id,
                "user_id": self.user_id,
                "start_time": datetime.now().isoformat()
            }
        }
        
        # æ‰§è¡ŒLangGraphå·¥ä½œæµ
        config = {
            "configurable": {
                "thread_id": self.conversation_id,
                "checkpoint_ns": f"agent_task_{self.conversation_id}"
            }
        }
        
        try:
            # æµå¼æ‰§è¡ŒLangGraph
            async for chunk in self.langgraph_manager.stream_workflow(initial_state, config):
                await self._process_langgraph_chunk(chunk)
            
            # è·å–æœ€ç»ˆçŠ¶æ€
            final_state = await self.langgraph_manager.get_final_state(config)
            await self._process_final_result(final_state)
            
        except Exception as e:
            self.logger.error(f"LangGraphæ‰§è¡Œå¤±è´¥: {str(e)}")
            await self.emit_error(
                error_code="LANGGRAPH_ERROR",
                error_message=f"æ™ºèƒ½ä»£ç†æ‰§è¡Œå¤±è´¥: {str(e)}"
            )
            raise
    
    async def _process_langgraph_chunk(self, chunk: Dict[str, Any]) -> None:
        """å¤„ç†LangGraphæµå¼è¾“å‡ºå—"""
        try:
            # è§£æchunkå†…å®¹
            node_name = chunk.get("node", "")
            node_output = chunk.get("output", {})
            
            if node_name:
                self.current_agent = node_name
                self.update_stage(f"agent_{node_name}")
                
                # è®°å½•æ‰§è¡Œæ­¥éª¤
                step = {
                    "agent": node_name,
                    "timestamp": datetime.now().isoformat(),
                    "output": node_output
                }
                self.execution_steps.append(step)
                
                # å‘é€çŠ¶æ€æ›´æ–°
                await self.emit_status(
                    f"agent_{node_name}",
                    agent_name=node_name,
                    metadata={"step_count": len(self.execution_steps)}
                )
                
                # å¤„ç†ä¸åŒAgentçš„è¾“å‡º
                await self._handle_agent_output(node_name, node_output)
                
        except Exception as e:
            self.logger.error(f"å¤„ç†LangGraph chunkå¤±è´¥: {str(e)}")
    
    async def _handle_agent_output(self, agent_name: str, output: Dict[str, Any]) -> None:
        """å¤„ç†ç‰¹å®šAgentçš„è¾“å‡º"""
        try:
            if agent_name == "master_agent":
                await self._handle_master_agent_output(output)
            elif agent_name == "query_optimizer":
                await self._handle_query_optimizer_output(output)
            elif agent_name == "parallel_search":
                await self._handle_parallel_search_output(output)
            elif agent_name == "summary_agent":
                await self._handle_summary_agent_output(output)
            elif agent_name == "final_output":
                await self._handle_final_output_output(output)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†Agent {agent_name} è¾“å‡ºå¤±è´¥: {str(e)}")
    
    async def _handle_master_agent_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æ€»æ§åˆ¶è€…Agentè¾“å‡º"""
        decision = output.get("decision", "")
        reasoning = output.get("reasoning", "")
        
        if reasoning:
            await self.emit_content(f"ğŸ¤– æ€»æ§åˆ¶è€…åˆ†æ: {reasoning}")
        
        if decision == "continue":
            await self.emit_content("éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯ï¼Œå¯åŠ¨æ£€ç´¢æµç¨‹...")
        elif decision == "finish":
            await self.emit_content("ä¿¡æ¯å……è¶³ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    
    async def _handle_query_optimizer_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†é—®é¢˜ä¼˜åŒ–Agentè¾“å‡º"""
        optimized_queries = output.get("optimized_queries", {})
        
        if optimized_queries:
            await self.emit_content("ğŸ” é—®é¢˜ä¼˜åŒ–å®Œæˆï¼Œç”Ÿæˆä¸“é—¨åŒ–æŸ¥è¯¢:")
            for agent_type, query in optimized_queries.items():
                await self.emit_content(f"  â€¢ {agent_type}: {query}")
    
    async def _handle_parallel_search_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†å¹¶è¡Œæœç´¢è¾“å‡º"""
        search_results = output.get("search_results", {})
        
        if search_results:
            await self.emit_content("ğŸ“Š å¹¶è¡Œæ£€ç´¢å®Œæˆ:")
            for search_type, results in search_results.items():
                result_count = len(results) if isinstance(results, list) else 1
                await self.emit_content(f"  â€¢ {search_type}: è·å¾— {result_count} ä¸ªç»“æœ")
    
    async def _handle_summary_agent_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æ‘˜è¦Agentè¾“å‡º"""
        summaries = output.get("summaries", {})
        
        if summaries:
            await self.emit_content("ğŸ“ ä¿¡æ¯æ‘˜è¦ç”Ÿæˆå®Œæˆ:")
            for source, summary in summaries.items():
                if summary:
                    await self.emit_content(f"  â€¢ {source}: {summary[:100]}...")
    
    async def _handle_final_output_output(self, output: Dict[str, Any]) -> None:
        """å¤„ç†æœ€ç»ˆè¾“å‡ºAgentè¾“å‡º"""
        final_answer = output.get("final_answer", "")
        
        if final_answer:
            # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²
            assistant_message = Message(
                role="assistant",
                content=final_answer,
                metadata={
                    "agent_mode": True,
                    "execution_steps": len(self.execution_steps),
                    "agents_used": list(set(step["agent"] for step in self.execution_steps))
                }
            )
            self.add_message(assistant_message)
            
            await self.emit_content(final_answer)
    
    async def _process_final_result(self, final_state: Dict[str, Any]) -> None:
        """å¤„ç†æœ€ç»ˆç»“æœ"""
        try:
            final_answer = final_state.get("final_answer", "")
            
            if final_answer and not any(msg.role == "assistant" for msg in self.history.messages[-1:]):
                # å¦‚æœè¿˜æ²¡æœ‰æ·»åŠ æœ€ç»ˆå›ç­”ï¼Œåˆ™æ·»åŠ 
                assistant_message = Message(
                    role="assistant",
                    content=final_answer,
                    metadata={
                        "agent_mode": True,
                        "final_state": True,
                        "execution_steps": len(self.execution_steps)
                    }
                )
                self.add_message(assistant_message)
            
            await self.emit_status("agent_workflow", status="completed", progress=1.0)
            
            self.logger.info(
                "Agentå·¥ä½œæµæ‰§è¡Œå®Œæˆ",
                conversation_id=self.conversation_id,
                execution_steps=len(self.execution_steps),
                final_answer_length=len(final_answer)
            )
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}")
            await self.emit_error(
                error_code="FINAL_RESULT_ERROR",
                error_message=f"å¤„ç†æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}"
            )
    
    def get_execution_summary(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        return {
            **self.get_conversation_summary(),
            "agent_mode": True,
            "current_agent": self.current_agent,
            "execution_steps": len(self.execution_steps),
            "agents_used": list(set(step["agent"] for step in self.execution_steps)),
            "global_context_summary": self.global_context.get_all_contexts_summary()
        }
